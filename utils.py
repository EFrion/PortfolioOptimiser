import json
import os
import yfinance as yf
from datetime import datetime, date
import csv
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from adjustText import adjust_text
import seaborn as sns
import matplotlib.pyplot as plt
from pandas.tseries.offsets import BDay

###
### Configuration initialisation section
###

# Load the configuration file
def load_config(config_file_path: str = 'config.json') -> dict:
    """
    Loads configuration settings from a JSON file.

    Args:
        config_file_path (str): The path to the configuration JSON file.

    Returns:
        dict: A dictionary containing the loaded configuration settings.
    """
    try:
        with open(config_file_path, 'r') as f:
            config = json.load(f)
        print(f"\nConfiguration loaded successfully from {config_file_path}")
        return config
    except FileNotFoundError:
        print(f"Error: Configuration file not found at {config_file_path}")
        print("Please ensure 'config.json' is in the same directory as the script.")
        exit(1) # Exit the script if config file is missing
    except json.JSONDecodeError as e:
        print(f"Error: Could not decode JSON from {config_file_path}. Please check file format.")
        print(f"Details: {e}")
        exit(1) # Exit the script if JSON is malformed
    except Exception as e:
        print(f"An unexpected error occurred while loading config: {e}")
        exit(1)

# Clear any existing output file
def clear_portfolio_results_csv(output_dir: str, filename: str):
    """
    Clears (overwrites) the specified CSV file, effectively preparing it for new data.
    Called once at the very beginning of script execution.

    Args:
        OUTPUT_DIR (str): Output directory.
        OUTPUT_FILENAME (str): The name to the CSV file to clear.
    """
    # The path to the CSV file to clear
    filepath = os.path.join(output_dir, filename)
    try:
        # Ensure the directory exists before trying to open the file
        os.makedirs(output_dir, exist_ok=True)
        
        with open(filepath, 'w', newline='') as csvfile:
            # Just opening in 'w' mode and immediately closing clears the file.
            pass
        print(f"\nCleared existing data in {filepath} for a new run.")
        
        # Check to confirm the file is indeed empty
        if os.path.exists(filepath) and os.path.getsize(filepath) == 0:
            print(f"Confirmation: {filepath} is empty.")
        else:
            print(f"Warning: {filepath} was not empty after clearing attempt. Check permissions or other operations.")

    except IOError as e:
        print(f"Error clearing CSV file {filepath}: {e}")
    except Exception as e:
        print(f"An unexpected error occurred while clearing the CSV: {e}")

    return filepath
    
# Load and initialise settings
def load_and_initialise_settings(config_file='config.json'):
    """
    Loads configuration, initialises paths, creates necessary directories,
    and prepares output files.

    Args:
        config_file (str): Path to the configuration JSON file.

    Returns:
        tuple: A tuple containing:
            - config (dict): The loaded configuration dictionary.
            - output_filepath (str): The full path to the portfolio results CSV.
            - usd_column_mapping (dict): Column mapping for USD stock data.
            - eur_column_mapping (dict): Column mapping for EUR stock data.
            - exchange_rate_column_mapping (dict): Column mapping for exchange rate data.
    """
    config = load_config(config_file)

    # Output Settings
    OUTPUT_DIR = config['output_settings']['OUTPUT_DIR']
    OUTPUT_FILENAME = config['output_settings']['OUTPUT_FILENAME']
    BACKTEST_OUTPUT_FILENAME = config['output_settings']['BACKTEST_OUTPUT_FILENAME']
    
    # Ensure output directory exists before any file operations
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # Ensures output files are cleared every time the script starts.
    output_filepath = clear_portfolio_results_csv(OUTPUT_DIR, OUTPUT_FILENAME)
    backtest_output_filepath = clear_portfolio_results_csv(OUTPUT_DIR, BACKTEST_OUTPUT_FILENAME)

    # --- Data Paths ---
    STOCK_ROOT_FOLDER = config['data_paths']['STOCK_ROOT_FOLDER']
    USD_FOLDER = os.path.join(STOCK_ROOT_FOLDER, 'USD')
    EUR_FOLDER = os.path.join(STOCK_ROOT_FOLDER, 'EUR')
    EXCHANGE_RATE_FILE = os.path.join(STOCK_ROOT_FOLDER, config['data_paths']['EXCHANGE_RATE_FILE_NAME'])

    use_yahoo_finance = config['data_source'].get('USE_YAHOO_FINANCE', False)
    if use_yahoo_finance:
        print("\nUsing Yahoo Finance for stock data. Local CSV files will be ignored.")
        #print(f"Tickers to be fetched: {config['data_source'].get('YAHOO_FINANCE_TICKERS', 'None specified')}")
        # Get the cache directory path
        yahoo_finance_cache_dir = config['data_source'].get('YAHOO_FINANCE_CACHE_DIR', 'yahoo_cache')
        print(f"Yahoo Finance data will be cached in: {yahoo_finance_cache_dir}")
    else:
        print("\nUsing local CSV files for stock data.")
        # Create main folder and subfolders if they don't exist
        for folder in [USD_FOLDER, EUR_FOLDER]:
            if not os.path.exists(folder):
                os.makedirs(folder)
                print(f"Created folder: {folder}.")
        if not os.path.exists(STOCK_ROOT_FOLDER):
            os.makedirs(STOCK_ROOT_FOLDER)
            print(f"Created folder: {STOCK_ROOT_FOLDER}.")

        print("Please ensure your stock CSV files are in the 'Stocks_Data/USD' or 'Stocks_Data/EUR' subfolders.")
        print(f"For EUR conversion, also place '{os.path.basename(EXCHANGE_RATE_FILE)}' (with 'Date' and 'Dernier' columns, European format) in '{STOCK_ROOT_FOLDER}'.")


    # --- Define Column Mappings for different currencies/formats ---
    usd_column_mapping = {
        'Date': 'Date',
        'Close/Last': 'Close/Last',
        'Open': 'Open',
        'High': 'High',
        'Low': 'Low'
    }

    # Corrected column names based on user's feedback
    eur_column_mapping = {
        'Date': 'Date',
        'Close/Last': 'Dernier', # Map 'Dernier' from EUR CSV to 'Close/Last' for internal use
        'Open': 'Ouv.',
        'High': ' Plus Haut', # Retain leading space as requested
        'Low': 'Plus Bas'
    }

    exchange_rate_column_mapping = {
        'Date': 'Date',
        'Close': 'Dernier' # The 'Dernier' column for the rate, assuming it's the closing rate
    }

    return config, output_filepath, backtest_output_filepath, usd_column_mapping, eur_column_mapping, exchange_rate_column_mapping

###
### Data pre-processing section
###

# Function to load exchange rate data
def load_exchange_rate_data(exchange_rate_file, column_mapping=None, decimal_separator='.'):
    """
    Loads historical EUR/USD exchange rate data.
    Assumes the file has 'Date' and 'Close' (or similar) columns.
    """
    if column_mapping is None:
        column_mapping = {
            'Date': 'Date',
            'Close': 'Close' # Default mapping for exchange rate close price
        }

    try:
        eur_usd_df = pd.read_csv(exchange_rate_file, decimal=decimal_separator)
        
        # Apply column mapping
        eur_usd_df = eur_usd_df.rename(columns={v: k for k, v in column_mapping.items()})

        # Convert 'Date' column to datetime objects, assuming European format if decimal_separator is ','
        if decimal_separator == ',':
            eur_usd_df['Date'] = pd.to_datetime(eur_usd_df['Date'], format='%d/%m/%Y', errors='coerce')
        else:
            eur_usd_df['Date'] = pd.to_datetime(eur_usd_df['Date'], errors='coerce') # Coerce errors will turn invalid dates into NaT
        
        eur_usd_df = eur_usd_df.dropna(subset=['Date']) # Drop rows where date conversion failed

        eur_usd_df = eur_usd_df.sort_values(by='Date')
        
        # Rename 'Close' to 'EURUSD_Rate' for clarity during merge
        if 'Close' not in eur_usd_df.columns:
            raise KeyError(f"Mapped 'Close' column not found in exchange rate file '{exchange_rate_file}'. Check column_mapping.")

        eur_usd_df = eur_usd_df[['Date', 'Close']].rename(columns={'Close': 'EURUSD_Rate'})
        print(f"Successfully loaded exchange rate data from {exchange_rate_file}")
        return eur_usd_df
    except FileNotFoundError:
        print(f"Warning: Exchange rate file '{exchange_rate_file}' not found. Cannot convert EUR stocks to USD.")
        return None
    except KeyError as e:
        print(f"Error: Exchange rate file '{exchange_rate_file}' must contain the expected columns. Details: {e}")
        return None
    except Exception as e:
        print(f"An error occurred loading exchange rate data: {e}")
        return None

# If the user prefers fetching data from Yahoo Finance, this function retrieves the data
def fetch_yahoo_finance_data(tickers: list, start_date: str, end_date: str, cache_dir: str = 'yahoo_cache'):
    """
    Fetches historical adjusted close price data from Yahoo Finance for given tickers.
    Caches the data locally and downloads only once per day if cached data is fresh.
    Removes Yahoo Finance exchange suffixes (e.g., .PA, .BR) from ticker names.

    Args:
        tickers (list): A list of ticker symbols (e.g., ["AAPL", "MSFT"]).
        start_date (str): The start date for fetching data (YYYY-MM-DD).
        end_date (str): The end date for fetching data (YYYY-MM-DD).
        cache_dir (str): Directory to store cached Yahoo Finance data.

    Returns:
        pd.DataFrame: A DataFrame with 'Date' as index and columns for each ticker's
                      Close price, or None if an error occurs.
    """
    # Create cache directory if it doesn't exist
    os.makedirs(cache_dir, exist_ok=True)
    cache_file_name = f"yahoo_finance_data_{start_date}_to_{end_date}.csv"
    cache_file_path = os.path.join(cache_dir, cache_file_name)

    # Convert start_date and end_date to datetime objects for comparison
    start_dt = datetime.strptime(start_date, '%Y-%m-%d').date()
    end_dt = datetime.strptime(end_date, '%Y-%m-%d').date()
    current_date = date.today()

    # --- Check if cached data is fresh ---
    if os.path.exists(cache_file_path):
        file_mod_time = datetime.fromtimestamp(os.path.getmtime(cache_file_path)).date()
        # Check if the file was modified today and covers the requested end date
        if file_mod_time == current_date and end_dt <= current_date:
            try:
                print(f"Loading Yahoo Finance data from cache: {cache_file_path}")
                cached_data = pd.read_csv(cache_file_path, index_col='Date', parse_dates=True)
                # Ensure all requested tickers are in the cached data, otherwise re-download
                if all(ticker.split('.')[0] in cached_data.columns for ticker in tickers):
                    # Filter data to the requested date range, just in case cache has more
                    cached_data = cached_data[(cached_data.index.date >= start_dt) & (cached_data.index.date <= end_dt)]
                    print("Successfully loaded data from cache. No new download needed today.")
                    return cached_data
                else:
                    print("Cached data does not contain all requested tickers. Re-downloading.")
            except Exception as e:
                print(f"Error loading from cache ({e}). Re-downloading data.")
    else:
        print("No Yahoo Finance cache found. Downloading new data.")
        
    print(f"\nFetching data from Yahoo Finance for tickers: {', '.join(tickers)} from {start_date} to {end_date}...")
    try:
        # Download data for all tickers
        data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=True)

        # Fetch adjusted price (i.e. stock-splits- and dividend-adjusted
        if 'Close' in data.columns:
            close_prices = data['Close']
        elif len(tickers) == 1:
            close_prices = data[['Close']].rename(columns={'Close': tickers[0]})
        else:
            print("Error: 'Close' column not found in Yahoo Finance data.")
            return None

        close_prices.index.name = 'Date'
        close_prices = close_prices.sort_index()
        
        # --- Clean ticker names by removing eventual suffixes ---
        cleaned_columns = {}
        for col in close_prices.columns:
            # Check if it's a multi-index column
            if isinstance(col, tuple):
                original_ticker_with_suffix = col[0] # The ticker is the first element in the tuple
            else:
                original_ticker_with_suffix = col

            cleaned_ticker = original_ticker_with_suffix.split('.')[0]
            cleaned_columns[col] = cleaned_ticker

        close_prices.rename(columns=cleaned_columns, inplace=True)
        #print(f"Cleaned ticker suffixes for {cleaned_ticker}.")
        # -------------------------------------------------------

        # Drop any columns (tickers) that are all NaN
        initial_num_columns = close_prices.shape[1]
        close_prices.dropna(axis=1, how='all', inplace=True)
        if close_prices.shape[1] < initial_num_columns:
            dropped_tickers = set(tickers) - set(close_prices.columns)
            print(f"Warning: Dropped tickers with no available data: {', '.join(dropped_tickers)}")

        if close_prices.empty:
            print("Error: No valid Close price data retrieved from Yahoo Finance for the specified period.")
            return None

        print("Successfully fetched Yahoo Finance data.")
        
        # Save to cache
        close_prices.to_csv(cache_file_path)
        print(f"Yahoo Finance data saved to cache: {cache_file_path}")
        
        return close_prices

    except Exception as e:
        print(f"An error occurred while fetching data from Yahoo Finance: {e}")
        return None
        
# Function to prepare data and calculate stocks' returns
def load_and_preprocess_stock_data(config, usd_column_mapping, eur_column_mapping, exchange_rate_column_mapping, yahoo_finance_cache_dir):
    """
    Loads and preprocesses all stock data (USD and EUR), merges them,
    aligns dates, fills missing values, and calculates daily returns.

    Args:
        config (dict): The loaded configuration dictionary.
        usd_column_mapping (dict): Column mapping for USD stock data.
        eur_column_mapping (dict): Column mapping for EUR stock data.
        exchange_rate_column_mapping (dict): Column mapping for exchange rate data.
        yahoo_finance_cache_dir (str): Directory to store cached Yahoo Finance data.
    Returns:
        tuple: A tuple containing:
            - daily_returns (pd.DataFrame): DataFrame of daily returns for all stocks.
            - portfolio_tickers (list): List of tickers included in the portfolio.
            - individual_stock_metrics (list): List of dictionaries with individual stock metrics.
            - annual_returns_array (np.array): Annualised mean returns for all stocks.
            - all_stock_prices (pd.DataFrame): DataFrame of all stock prices after preprocessing.
            - BACKTEST_START_DATE (pd.Timestamp): The determined backtest start date.
            - BACKTEST_END_DATE (pd.Timestamp): The determined backtest end date.
    """
    STOCK_ROOT_FOLDER = config['data_paths']['STOCK_ROOT_FOLDER']
    USD_FOLDER = os.path.join(STOCK_ROOT_FOLDER, 'USD')
    EUR_FOLDER = os.path.join(STOCK_ROOT_FOLDER, 'EUR')
    EXCHANGE_RATE_FILE = os.path.join(STOCK_ROOT_FOLDER, config['data_paths']['EXCHANGE_RATE_FILE_NAME'])
    HISTORICAL_DATA_WINDOW_DAYS = config['backtesting_parameters']['HISTORICAL_DATA_WINDOW_DAYS']
    RUN_BACKTEST = config['feature_toggles']['RUN_BACKTEST']
    
    use_yahoo_finance = config['data_source'].get('USE_YAHOO_FINANCE', False)
    yahoo_finance_tickers = config['data_source'].get('YAHOO_FINANCE_TICKERS', [])
    fetch_start_date = config['data_source'].get('YAHOO_START_DATE', "")

                                       
    # individual_stock_metrics will now only store tickers and their full history metrics for initial printout
    individual_stock_metrics = [] 
    all_stock_prices = pd.DataFrame() # To store all stock prices for merging
    earliest_dates_per_stock = {} # To store the earliest valid date for each stock
    most_recent_dates_per_stock = {} # To store the most recent valid date for each stock
    
    if use_yahoo_finance:
        if not yahoo_finance_tickers:
            print("Error: 'USE_YAHOO_FINANCE' is true, but 'YAHOO_FINANCE_TICKERS' is empty. Please specify tickers or set 'USE_YAHOO_FINANCE' to false.")
            exit()

        fetch_end_date = pd.Timestamp.today()

        temp_yahoo_prices = fetch_yahoo_finance_data(
            yahoo_finance_tickers,
            fetch_start_date,
            fetch_end_date.strftime('%Y-%m-%d'),
            cache_dir=yahoo_finance_cache_dir
        )
        
        if temp_yahoo_prices is not None and not temp_yahoo_prices.empty:
            all_stock_prices = temp_yahoo_prices.reset_index() # Convert 'Date' index back to column
            all_stock_prices.rename(columns={'index': 'Date'}, inplace=True) # Ensure 'Date' column is named correctly

            # Calculate individual stock metrics from Yahoo Finance data for printing
            for ticker_with_suffix in yahoo_finance_tickers:
                # Get the cleaned ticker name that will be present in all_stock_prices columns
                cleaned_ticker = ticker_with_suffix.split('.')[0]

                if cleaned_ticker in all_stock_prices.columns:
                    stock_df = all_stock_prices[['Date', cleaned_ticker]].dropna()
                    if not stock_df.empty and len(stock_df) > 1:
                        stock_df = stock_df.sort_values(by='Date')
                        daily_returns = stock_df[cleaned_ticker].pct_change().dropna()
                        annualised_std_dev = daily_returns.std() * np.sqrt(252)
                        annualised_avg_return = daily_returns.mean() * 252

                        beginning_value = stock_df[cleaned_ticker].iloc[0]
                        ending_value = stock_df[cleaned_ticker].iloc[-1]
                        beginning_date = stock_df['Date'].iloc[0]
                        ending_date = stock_df['Date'].iloc[-1]
                        number_of_years = (ending_date - beginning_date).days / 365.25
                        cagr = (ending_value / beginning_value)**(1 / number_of_years) - 1 if number_of_years > 0 else np.nan
                        last_price = stock_df[cleaned_ticker].iloc[-1]

                        individual_stock_metrics.append({
                            'ticker': cleaned_ticker,
                            'annualised_std': annualised_std_dev,
                            'cagr': cagr,
                            'annual_return': annualised_avg_return,
                            'last_price': last_price
                        })
                        earliest_dates_per_stock[cleaned_ticker] = beginning_date
                        most_recent_dates_per_stock[cleaned_ticker] = ending_date
                    else:
                        print(f"Warning: Not enough valid data points for {cleaned_ticker} from Yahoo Finance for calculations. Aborting.")
                        exit()
                else:
                    print(f"Warning: Ticker '{ticker_with_suffix}' (cleaned to '{cleaned_ticker}') not found in fetched Yahoo Finance data. Aborting.")
                    exit()
        else:
            print("Error: Could not retrieve any valid data from Yahoo Finance. Aborting.")
            exit()

    else: # Continue with CSV loading logic
        # --- Load Exchange Rate Data ---
        eur_usd_rates_df = load_exchange_rate_data(EXCHANGE_RATE_FILE,
                                                column_mapping=exchange_rate_column_mapping,
                                                decimal_separator=',')

        # --- Process USD Stocks ---
        if os.path.exists(USD_FOLDER):
            print(f"\nProcessing stocks in {USD_FOLDER}...")
            for filename in os.listdir(USD_FOLDER):
                if filename.endswith('.csv'):
                    file_path = os.path.join(USD_FOLDER, filename)
                    ticker = os.path.splitext(filename)[0].upper()
                    stock_price_df, annual_std, cagr, annual_avg_ret, _, last_price, earliest_date_for_stock, most_recent_date_per_stock = \
                        process_single_stock_data(file_path, ticker, is_eur_stock=False,
                                                column_mapping=usd_column_mapping, decimal_separator='.')
                    if stock_price_df is not None:
                        individual_stock_metrics.append({
                            'ticker': ticker,
                            'annualised_std': annual_std,
                            'cagr': cagr,
                            'annual_return': annual_avg_ret,
                            'last_price': last_price
                        })
                        if all_stock_prices.empty:
                            all_stock_prices = stock_price_df
                        else:
                            all_stock_prices = pd.merge(all_stock_prices, stock_price_df, on='Date', how='outer')
                        earliest_dates_per_stock[ticker] = earliest_date_for_stock
                        most_recent_dates_per_stock[ticker] = most_recent_date_per_stock
        else:
            print(f"Folder '{USD_FOLDER}' not found. Skipping USD stock processing.")

        # --- Process EUR Stocks ---
        if os.path.exists(EUR_FOLDER):
            print(f"\nProcessing stocks in {EUR_FOLDER}...")
            if eur_usd_rates_df is None:
                print("Cannot process EUR stocks as EUR/USD exchange rate data is not available.")
            else:
                for filename in os.listdir(EUR_FOLDER):
                    if filename.endswith('.csv'):
                        file_path = os.path.join(EUR_FOLDER, filename)
                        ticker = os.path.splitext(filename)[0].upper()
                        stock_price_df, annual_std, cagr, annual_avg_ret, _, last_price, earliest_date_for_stock, most_recent_date_per_stock = \
                            process_single_stock_data(file_path, ticker, is_eur_stock=True, eur_usd_df=eur_usd_rates_df.copy(),
                                                    column_mapping=eur_column_mapping, decimal_separator=',')
                        if stock_price_df is not None:
                            individual_stock_metrics.append({
                                'ticker': ticker,
                                'annualised_std': annual_std,
                                'cagr': cagr,
                                'annual_return': annual_avg_ret,
                                'last_price': last_price
                            })
                            if all_stock_prices.empty:
                                all_stock_prices = stock_price_df
                            else:
                                all_stock_prices = pd.merge(all_stock_prices, stock_price_df, on='Date', how='outer')
                            earliest_dates_per_stock[ticker] = earliest_date_for_stock
                            most_recent_dates_per_stock[ticker] = most_recent_date_per_stock
        else:
            print(f"Folder '{EUR_FOLDER}' not found. Skipping EUR stock processing.")
    
    # Sort the list of stock metrics alphabetically by ticker
    individual_stock_metrics.sort(key=lambda x: x['ticker'])

    # --- Finalise price data and calculate daily returns for all stocks ---
    if all_stock_prices.empty:
        print(f'Error: no stock price data collected. Cannot proceed with portfolio optimisation.')
        exit()
        
    # Sort all_stock_prices chronologically (.set_index('Date').sort_index()) and alphabetically (.sort_index(axis=1))
    all_stock_prices = all_stock_prices.set_index('Date').sort_index().sort_index(axis=1)
    
    # --- Determine global common start and end dates, then truncate ---
    global_common_start_date = None
    global_common_end_date = None
    
    if earliest_dates_per_stock:
        global_common_start_date = max(earliest_dates_per_stock.values())
        print(f"\nGlobal common start date for all loaded stocks: {global_common_start_date.strftime('%Y-%m-%d')}")
    
        # Truncate all_stock_prices to start from this common date
        all_stock_prices = all_stock_prices[all_stock_prices.index >= global_common_start_date]
        #print(f"New all_stock_prices shape after common start date truncation: {all_stock_prices.shape}")
        
    if most_recent_dates_per_stock:
        global_common_end_date = max(most_recent_dates_per_stock.values())
        print(f"\nGlobal common end date for all loaded stocks: {global_common_end_date.strftime('%Y-%m-%d')}")


    if RUN_BACKTEST:
        # Set BACKTEST_START_DATE to the global common start date by default
        BACKTEST_START_DATE = config['backtesting_parameters']['BACKTEST_START_DATE']
        BACKTEST_END_DATE = config['backtesting_parameters']['BACKTEST_END_DATE']

        if BACKTEST_START_DATE=='':
            if global_common_start_date:
                BACKTEST_START_DATE = global_common_start_date.strftime('%Y-%m-%d')
                print(f"\nSet BACKTEST_START_DATE to {BACKTEST_START_DATE}.")
            else:
                print(f"BACKTEST_START_DATE undefined and no global common start date. Aborting backtest.")
                exit()
        else:
            # If user set an early date, it will be effectively limited by global_common_start_date.
            BACKTEST_START_DATE = pd.to_datetime(BACKTEST_START_DATE)
            if global_common_start_date and BACKTEST_START_DATE < global_common_start_date:
                print(f"Warning: Configured BACKTEST_START_DATE ({BACKTEST_START_DATE.strftime('%Y-%m-%d')}) is earlier than global common data start ({global_common_start_date.strftime('%Y-%m-%d')}). Using global common start date.")
                BACKTEST_START_DATE = global_common_start_date.strftime('%Y-%m-%d')
            print(f"Using BACKTEST_START_DATE: {BACKTEST_START_DATE}.")
            
        if BACKTEST_END_DATE == '':
            if global_common_end_date:
                BACKTEST_END_DATE = global_common_end_date.strftime('%Y-%m-%d')
                print(f"Set BACKTEST_END_DATE to {BACKTEST_END_DATE}.")
            else:
                print(f"BACKTEST_END_DATE undefined and no global common end date. Aborting backtest.")
                exit()
        else:
            BACKTEST_END_DATE = pd.to_datetime(BACKTEST_END_DATE)
            if global_common_end_date and BACKTEST_END_DATE > global_common_end_date:
                print(f"Warning: Configured BACKTEST_END_DATE ({BACKTEST_END_DATE.strftime('%Y-%m-%d')}) is later than global common data end ({global_common_end_date.strftime('%Y-%m-%d')}). Using global common end date.")
                BACKTEST_END_DATE = global_common_end_date.strftime('%Y-%m-%d')
            print(f"Using BACKTEST_END_DATE: {BACKTEST_END_DATE}.")
        
        # Ensure we have datetime objects for consistency
        BACKTEST_START_DATE = pd.to_datetime(BACKTEST_START_DATE)
        BACKTEST_END_DATE = pd.to_datetime(BACKTEST_END_DATE)
    else:
        BACKTEST_START_DATE = ""
        BACKTEST_END_DATE = ""
    
    # Apply ffill() and bfill() to fill any *remaining* internal missing values
    #print("\nDEBUG: Applying ffill() and bfill() to fill any internal missing price data...")
    all_stock_prices = all_stock_prices.ffill().bfill()

    # Drop columns (tickers) that are still entirely NaN after filling (meaning no data at all even after common start date)
    initial_num_columns = all_stock_prices.shape[1]
    all_stock_prices.dropna(axis=1, how='all', inplace=True)
    if all_stock_prices.shape[1] < initial_num_columns:
        # Identify dropped columns by comparing sets of columns
        print(f"Dropped {initial_num_columns - all_stock_prices.shape[1]} columns due to all NaN values after filling.")

    #print(f"\nDEBUG: all_stock_prices after common start date truncation, ffill, bfill, and dropping all-NaN columns:")
    #print(f"DEBUG: Shape: {all_stock_prices.shape}")
    #print(f"DEBUG: Is empty: {all_stock_prices.empty}")
    if not all_stock_prices.empty:
        print("\nHead of all_stock_prices:")
        print(all_stock_prices.head())
        print("\nTail of all_stock_prices:")
        print(all_stock_prices.tail())
    else:
        print("DEBUG: all_stock_prices is EMPTY after cleaning.")
        print("Please check your input CSV files for overlapping dates and complete data.")
        exit()

    # Now calculate daily returns from the aligned price data
    daily_returns = all_stock_prices.pct_change().dropna()

    #print(f"\nDEBUG: daily_returns after pct_change().dropna():")
    #print(f"DEBUG: Shape: {daily_returns.shape}")
    #print(f"DEBUG: Is empty: {daily_returns.empty}")
    if not daily_returns.empty:
        print("\nHead of daily_returns:")
        print(daily_returns.head())
        print("\nTail of daily_returns:")
        print(daily_returns.tail())
    else:
        print("DEBUG: daily_returns is EMPTY. This will prevent further calculations.")
        print("Please ensure your data has at least two trading days with valid prices for all selected stocks.")
        exit() # Exit early if no daily returns

    # Ensure we have enough data after merging and cleaning
    if len(daily_returns.columns) < 2:
        print("Error: Not enough common valid stock data (min 2) across all files after date alignment. Plotting cannot proceed.")
        print("Please check your CSV files for overlapping dates and valid price data.")
        exit()

    portfolio_tickers = sorted(list(daily_returns.columns))
    #print(portfolio_tickers)
    num_assets = len(portfolio_tickers)

    # --- Calculate the annualised mean annual_returns_array based on the common daily_returns times the average number of trading days ---
    annual_returns_array = daily_returns.mean() * 252
    annual_returns_array = annual_returns_array.values # Convert to numpy array for optimisation functions
    
    print("\n--- Individual Stock Metrics (All in USD) ---")
    # Using the metrics from the full individual history for this printout, as it's descriptive.
    # The optimisation will use the aligned annual_returns_array.
    for stock in individual_stock_metrics:
        print(f"Ticker: {stock['ticker']}")
        print(f"  Annualised Return: {stock['annual_return']:.2%}")
        print(f"  Annualised Std Dev: {stock['annualised_std']:.2%}")
        print(f"  CAGR: {stock['cagr']:.2%}")
        print(f"  Last Close Price (USD): {stock['last_price']:.2f}\n")
           
    return daily_returns, portfolio_tickers, individual_stock_metrics, annual_returns_array, all_stock_prices, BACKTEST_START_DATE, BACKTEST_END_DATE

# Function to retrieve data from a single stock
def process_single_stock_data(file_path, ticker, is_eur_stock=False, eur_usd_df=None, column_mapping=None, decimal_separator='.'):
    """
    Loads stock price data from a CSV, cleans it, and performs currency conversion if needed.
    Returns the DataFrame with 'Date' and 'Close/Last' (adjusted for currency if applicable).

    Args:
        file_path (str): The full path to the input CSV file.
        ticker (str): The ticker symbol for the stock.
        is_eur_stock (bool): True if the stock is denominated in EUR and needs conversion.
        eur_usd_df (pd.DataFrame, optional): DataFrame containing EUR/USD exchange rates.
        column_mapping (dict, optional): A dictionary to map standard column names
                                        (e.g., 'Date', 'Close/Last') to actual names
                                        in the CSV file.
        decimal_separator (str): The decimal separator used in the CSV ('.' or ',').

    Returns:
        pd.DataFrame: DataFrame with 'Date' and 'Close/Last' (price) columns, or None if error.
        float: Annualised standard deviation.
        float: Compound Annual Growth Rate (CAGR).
        float: Annualised average return.
        str: Ticker symbol.
        float: The last closing price of the stock.
        pd.Timestamp: The earliest valid date for this stock.
    """
    if column_mapping is None:
        column_mapping = {
            'Date': 'Date',
            'Close/Last': 'Close/Last',
            'Open': 'Open',
            'High': 'High',
            'Low': 'Low'
        }

    try:
        df = pd.read_csv(file_path, decimal=decimal_separator)
        df = df.rename(columns={v: k for k, v in column_mapping.items()})

        for col in ['Close/Last', 'Open', 'High', 'Low']:
            if col in df.columns:
                df[col] = df[col].astype(str).replace({r'\$': '', 'â‚¬': ''}, regex=True).astype(float)
            else:
                if col in column_mapping:
                    raise KeyError(f"Missing expected column '{column_mapping[col]}' (mapped to '{col}') in file: {file_path}")

        if is_eur_stock and 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')
        else:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

        df = df.dropna(subset=['Date'])
        df = df.sort_values(by='Date')

        if is_eur_stock and eur_usd_df is not None:
            original_rows = len(df)
            df = pd.merge(df, eur_usd_df, on='Date', how='inner')
            if len(df) < original_rows:
                print(f"Warning: Missing exchange rate data for some dates for {ticker}. Filtered to {len(df)} common dates.")
            if df.empty:
                print(f"Error: No overlapping dates between {ticker} and EUR/USD exchange rates. Cannot process.")
                return None, None, None, None, None, None, None, None # Added None for earliest_date
            df['Close/Last'] = df['Close/Last'] * df['EURUSD_Rate']
            print(f"Converted {ticker} (EUR) prices to USD.")

        # Calculate metrics for individual stock (for printing, not for backtest loop directly)
        daily_returns = df['Close/Last'].pct_change().dropna()
        if len(daily_returns) < 2:
            print(f"Warning: Not enough valid data points for {ticker} after cleaning for volatility/return calculations.")
            return None, None, None, None, None, None, None, None # Added None for earliest_date

        annualised_std_dev = daily_returns.std() * np.sqrt(252)
        annualised_avg_return = daily_returns.mean() * 252

        beginning_value = df['Close/Last'].iloc[0]
        ending_value = df['Close/Last'].iloc[-1]
        beginning_date = df['Date'].iloc[0]
        ending_date = df['Date'].iloc[-1]
        number_of_years = (ending_date - beginning_date).days / 365.25
        cagr = (ending_value / beginning_value)**(1 / number_of_years) - 1 if number_of_years > 0 else np.nan
        
        last_close_price = df['Close/Last'].iloc[-1] if not df['Close/Last'].empty else np.nan
        
        earliest_valid_date = df['Date'].min() # Get the earliest date for this specific stock
        most_recent_valid_date = df['Date'].max() # Get the most recent date for this specific stock

        return df[['Date', 'Close/Last']].rename(columns={'Close/Last': ticker}), annualised_std_dev, cagr, annualised_avg_return, ticker, last_close_price, earliest_valid_date, most_recent_valid_date

    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
        return None, None, None, None, None, None, None, None
    except KeyError as e:
        print(f"Error: Missing expected column in '{file_path}'. Please check column names and mapping. Details: {e}")
        return None, None, None, None, None, None, None, None
    except pd.errors.EmptyDataError:
        print(f"Error: The file '{file_path}' is empty.")
        return None, None, None, None, None, None, None, None
    except Exception as e:
        print(f"An unexpected error occurred processing '{file_path}': {e}")
        return None, None, None, None, None, None, None, None

###
### Constraints sector
###

# Function setting portfolio constraints
def setup_optimisation_constraints(num_assets, portfolio_tickers, stock_sectors, configured_max_stock_weight, configured_max_sector_weight, run_mvo_optimisation):
    """
    Sets up the bounds and constraints for portfolio optimisation.

    Args:
        num_assets (int): The total number of assets in the portfolio.
        portfolio_tickers (list): A list of ticker symbols for the assets.
        stock_sectors (dict): A dictionary mapping stock tickers to their sectors.
        configured_max_stock_weight (float): Maximum allowed weight for a single stock.
        configured_max_sector_weight (float): Maximum allowed weight for a single sector.
        run_mvo_optimisation (bool): check if efficient frontier should be traced or not

    Returns:
        tuple: A tuple containing:
            - bounds (tuple): Bounds for each asset's weight (0 to 1).
            - constraints (list): A list of dictionaries defining the optimisation constraints.
            - initial_guess (np.array): An initial equal-weighted guess for optimisation.
    """

    bounds = tuple((0, 1) for asset in range(num_assets)) # Weights between 0 and 1 (no short-selling)
    
    
    # Base constraint: sum of weights equals 1
    constraints = [
        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}
    ]

    # Ideally, individual stock weight constraint should be at most 5%.
    effective_max_stock_weight = max(configured_max_stock_weight, 1.0 / num_assets)
    print(f"Maximum individual stock weight: {effective_max_stock_weight:.2%}\n")
    if num_assets <= 20 and run_mvo_optimisation:
        print("WARNING: with 20 assets or less, the efficient frontier is reduced to a single point (MVP) because the code currently constrains each asset being at most 5% of your portfolio.")
    elif num_assets > 20 and run_mvo_optimisation:
        print("More than 20 assets detected. We will attempt to draw the efficient frontier.\n")

    # Add individual stock weight constraint (max 5%)
    for i in range(num_assets):
        constraints.append({
            'type': 'ineq',
            'fun': lambda x, idx=i: effective_max_stock_weight - x[idx]
        })

    # Add sector weight constraints (max 25%)
    sectors = {}
    for i, ticker in enumerate(portfolio_tickers):
        sector = stock_sectors.get(ticker)
        if sector:
            if sector not in sectors:
                sectors[sector] = []
            sectors[sector].append(i) # Store index of stock in portfolio_tickers
        else:
            print(f"Warning: Ticker '{ticker}' not found in stock_sectors. It will not be subject to sector constraints.")

    # Determine a dynamic maximum sector weight based on the number of assets
    # If num_assets is less than 20, allow sectors to take up to 100% (effectively disabling the hard cap)
    effective_max_sector_weight_for_constraint = configured_max_sector_weight
    if num_assets <= 20:
        effective_max_sector_weight_for_constraint = 1.0 # Allow up to 100% for sectors if few assets
        #print(f"effective_max_sector_weight_for_constraint:{effective_max_sector_weight_for_constraint}") 
    
    
    for sector_name, stock_indices in sectors.items():
        # Maximum possible weight a sector can have given individual stock limits.
        sum_of_effective_max_stock_weights_in_sector = sum(effective_max_stock_weight for _ in stock_indices)
        
        # Determine the effective maximum sector weight for this specific sector.
        current_sector_limit = min(effective_max_sector_weight_for_constraint, sum_of_effective_max_stock_weights_in_sector)
        
        print(f"Maximum weight for sector '{sector_name}': {current_sector_limit:.2%}")

        constraints.append({
            'type': 'ineq',
            'fun': lambda x, indices=stock_indices, effective_limit=current_sector_limit: effective_limit - np.sum(x[indices]),
            'args': (stock_indices, current_sector_limit,)
        })

    # Set to equal-weighted portfolio
    initial_guess = np.array(num_assets * [1. / num_assets])
    
    return bounds, constraints, initial_guess

###
### Portfolio optimisation section
###

# Calculate risk (volatility)
def portfolio_volatility(weights, annualised_covariance_matrix):
    """
    Objective function to minimise: Portfolio Standard Deviation (Volatility).
    """
    return np.sqrt(np.dot(weights.T, np.dot(annualised_covariance_matrix, weights)))

# Calculate return
def portfolio_return(weights, annual_returns):
    """
    Calculates the portfolio's expected return.
    """
    return np.sum(weights * annual_returns)

# Calculate portfolio metrics
def calculate_portfolio_metrics(weights, annual_returns, annualised_covariance_matrix):
    """
    Calculates portfolio return and standard deviation for a given set of weights.

    Args:
        weights (np.array): Array of weights for each asset.
        annual_returns (np.array): Array of annualised returns for each asset.
        annualised_covariance_matrix (np.array): Annualised covariance matrix of asset returns.

    Returns:
        tuple: (portfolio_return, portfolio_std_dev)
    """
    portfolio_ret = np.sum(weights * annual_returns)
    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(annualised_covariance_matrix, weights)))
    return portfolio_ret, portfolio_std

# Define the negative Sharpe ratio. This is the function to minimise
def negative_sharpe_ratio(weights, annual_returns, annualised_covariance_matrix, risk_free_rate):
    """
    Objective function to minimise for finding the Tangency Portfolio:
    Negative Sharpe Ratio.
    """
    p_return = portfolio_return(weights, annual_returns)
    p_volatility = portfolio_volatility(weights, annualised_covariance_matrix)
    if p_volatility == 0:
        return -np.inf # Avoid division by zero, return negative infinity for extremely good Sharpe
    return -(p_return - risk_free_rate) / p_volatility

# Calculate only undesired volatility (downside risk)
def downside_deviation(weights, daily_returns_df_slice, risk_free_rate):
    """
    Calculates the annualised downside deviation for a portfolio.
    Only considers returns below the Minimum Acceptable Return (MAR), which is the risk-free rate.
    
    Args:
        weights (np.array): Array of weights for each asset.
        daily_returns_df_slice (pd.DataFrame): Daily returns for the lookback period.
        risk_free_rate (float): Annualised risk-free rate.
    """
    # Calculate portfolio daily returns for the slice
    portfolio_daily_returns = daily_returns_df_slice.dot(weights)
    
    # Calculate daily MAR
    daily_mar = (1 + risk_free_rate)**(1/252) - 1 # Convert annualised risk-free rate to daily

    # Filter for returns below the MAR
    downside_returns = portfolio_daily_returns[portfolio_daily_returns < daily_mar]
    
    if downside_returns.empty:
        return 0.0 # No downside returns, so downside deviation is 0

    # Calculate downside deviation (standard deviation of downside returns)
    downside_std = np.sqrt(np.mean((downside_returns - daily_mar)**2))
    
    # Annualise downside deviation
    annualised_downside_std = downside_std * np.sqrt(252)
    return annualised_downside_std

# Similar to the negative Sharpe ratio but for downside volatility 
def negative_sortino_ratio(weights, annual_returns, daily_returns_df_slice, annualised_covariance_matrix, risk_free_rate):
    """
    Objective function to minimise for finding the Sortino Ratio optimised portfolio:
    Negative Sortino Ratio.
    """
    p_return = portfolio_return(weights, annual_returns)
    # Pass the relevant slice of daily returns to downside_deviation
    p_downside_dev = downside_deviation(weights, daily_returns_df_slice, risk_free_rate)
    
    if p_downside_dev == 0:
        return -np.inf # Avoid division by zero, return negative infinity for extremely good Sortino
    return -(p_return - risk_free_rate) / p_downside_dev

# Calculate the "asymmetry" of returns distribution
def portfolio_skewness(weights, daily_returns_df_slice):
    """
    Calculates the skewness for a portfolio's daily returns.
    """
    portfolio_daily_returns = daily_returns_df_slice.dot(weights)
    return portfolio_daily_returns.skew()

# Calculate extreme events in returns distribution
def portfolio_kurtosis(weights, daily_returns_df_slice):
    """
    Calculates the kurtosis for a portfolio's daily returns.
    """
    portfolio_daily_returns = daily_returns_df_slice.dot(weights)
    return portfolio_daily_returns.kurtosis()

# Function to minimise
def negative_mvsk_utility(weights, annual_returns, daily_returns_df_slice, annualised_covariance_matrix, risk_free_rate, lambda_s, lambda_k):
    """
    Objective function to minimise for Mean-Variance-Skewness-Kurtosis (MVSK) optimisation.
    Maximises a utility function that considers mean, variance, skewness, and kurtosis.
    
    Args:
        weights (np.array): Portfolio weights.
        annual_returns (np.array): Annualised expected returns of assets.
        daily_returns_df_slice (pd.DataFrame): Daily returns for the lookback period.
        annualised_covariance_matrix (np.array): Annualised covariance matrix.
        risk_free_rate (float): Risk-free rate.
        lambda_s (float): Coefficient for skewness (positive to prefer higher skewness).
        lambda_k (float): Coefficient for kurtosis (positive to prefer lower kurtosis).
    """
    p_return = portfolio_return(weights, annual_returns)
    p_volatility = portfolio_volatility(weights, annualised_covariance_matrix)
    p_skewness = portfolio_skewness(weights, daily_returns_df_slice)
    p_kurtosis = portfolio_kurtosis(weights, daily_returns_df_slice)

    if p_volatility == 0:
        return np.inf 

    # A common MVSK objective for maximisation (we minimise the negative):
    utility = (p_return - risk_free_rate) / p_volatility + lambda_s * p_skewness - lambda_k * p_kurtosis
    
    return -utility

# Calculate the maximum drawdown
def calculate_max_drawdown(cumulative_returns_series: pd.Series) -> float:
    """
    Calculates the maximum drawdown for a series of cumulative returns.

    Args:
        cumulative_returns_series (pd.Series): A Pandas Series of cumulative returns, where the first value is typically 1.0.

    Returns:
        float: The maximum drawdown as a negative percentage (e.g., -0.15 for 15% drawdown).
               Returns 0.0 if the series is empty or has no drawdown.
    """
    if cumulative_returns_series.empty:
        return 0.0

    # Calculate the running maximum (peak)
    running_max = cumulative_returns_series.cummax()
    
    # Calculate the drawdown from the running maximum
    drawdown = (cumulative_returns_series / running_max) - 1.0
    
    # The maximum drawdown is the minimum (most negative) value in the drawdown series
    max_drawdown = drawdown.min()
    
    return max_drawdown
    
def _run_single_optimisation(objective_function, initial_guess, args, bounds, constraints):
    """
    Helper function to run a single optimisation using scipy.optimize.minimize.
    
    Args:
        objective_function (callable): The function to minimise.
        initial_guess (np.ndarray): Initial guess for weights.
        args (tuple): Additional arguments to pass to the objective function.
        bounds (tuple): Bounds for each weight.
        constraints (tuple): Constraints for the optimisation.
        
    Returns:
        tuple: (optimal_weights, success_status, message)
    """
    result = minimize(objective_function, initial_guess, args=args,
                      method='SLSQP', bounds=bounds, constraints=constraints)
    
    if result.success and not np.any(np.isnan(result.x)):
        return result.x, True, result.message
    else:
        return initial_guess, False, result.message # Return initial_guess as fallback

def _calculate_portfolio_metrics_full(weights, annual_returns, daily_returns_df_slice, annualised_covariance_matrix, risk_free_rate, lambda_s=None, lambda_k=None):
    """
    Calculates a comprehensive set of portfolio metrics.
    
    Args:
        weights (np.array): Array of weights for each asset.
        annual_returns (np.array): Array of annualised returns for each asset.
        daily_returns_df_slice (pd.DataFrame): Daily returns for the lookback period.
        annualised_covariance_matrix (np.array): Annualised covariance matrix of asset returns.
        risk_free_rate (float): Risk-free rate.
        lambda_s (float, optional): Coefficient for skewness (for MVSK).
        lambda_k (float, optional): Coefficient for kurtosis (for MVSK).
        
    Returns:
        dict: A dictionary of calculated metrics.
    """
    metrics = {}
    
    p_return = portfolio_return(weights, annual_returns)
    p_volatility = portfolio_volatility(weights, annualised_covariance_matrix)
    
    metrics['Return'] = p_return
    metrics['Volatility'] = p_volatility
    
    if p_volatility > 0:
        metrics['Sharpe Ratio'] = (p_return - risk_free_rate) / p_volatility
    else:
        metrics['Sharpe Ratio'] = np.inf if p_return > risk_free_rate else np.nan # Handle zero volatility

    p_downside_dev = downside_deviation(weights, daily_returns_df_slice, risk_free_rate)
    if p_downside_dev > 0:
        metrics['Sortino Ratio'] = (p_return - risk_free_rate) / p_downside_dev
    else:
        metrics['Sortino Ratio'] = np.inf if p_return > risk_free_rate else np.nan # Handle zero downside deviation

    metrics['Skewness'] = portfolio_skewness(weights, daily_returns_df_slice)
    metrics['Kurtosis'] = portfolio_kurtosis(weights, daily_returns_df_slice)
    
    # MVSK Utility
    if lambda_s is not None and lambda_k is not None:
        if p_volatility > 0:
            metrics['MVSK Utility'] = (p_return - risk_free_rate) / p_volatility + lambda_s * metrics['Skewness'] - lambda_k * metrics['Kurtosis']
        else:
            metrics['MVSK Utility'] = np.inf if p_return > risk_free_rate else np.nan
            
    return metrics
    
def calculate_mvp_portfolio(annual_returns: np.ndarray, 
                            covariance_matrix: np.ndarray, 
                            initial_guess: np.ndarray, 
                            bounds: tuple, 
                            constraints: tuple,
                            daily_returns_df_slice: pd.DataFrame,
                            risk_free_rate: float,
                            num_assets: int,
                            num_frontier_points: int,
                            verbose: bool = True) -> dict:
    """
    Calculates the Minimum Variance Portfolio (MVP) and traces the Efficient Frontier
    for a given static set of returns and covariance.

    Args:
        annual_returns (np.ndarray): Array of annualised returns for each asset.
        covariance_matrix (np.ndarray): Annualised covariance matrix of asset returns.
        initial_guess (np.ndarray): Initial guess for portfolio weights.
        bounds (tuple): Tuple of (min_weight, max_weight) for each asset.
        daily_returns_df_slice (pd.DataFrame): Daily returns for the lookback period (for metrics).
        risk_free_rate (float): Risk-free rate (for metrics).
        constraints (tuple): Tuple of constraints for the optimisation.
        num_assets (int): Number of assets in the portfolio.
        num_frontier_points (int): Number of points to calculate for the efficient frontier.

    Returns:
        dict: A dictionary containing MVP details and efficient frontier data.
    """
    results = {
        'weights': initial_guess,
        'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
        'success': False,
        'message': "Optimisation not attempted or failed",
        'efficient_frontier_std_devs': [],
        'efficient_frontier_returns': []
    }

    # Check if covariance matrix is singular
    if np.linalg.matrix_rank(covariance_matrix) < covariance_matrix.shape[0]:
        message = "Optimisation defaulted to initial weights due to singular covariance matrix."
        if verbose:
            print(f"Warning: {message}")
            return {
                'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate, lambda_s, lambda_k),
                'success': True, # Treat as success for test reporting, as it defaults to valid weights
                'message': message,
                'efficient_frontier_std_devs': [],
                'efficient_frontier_returns': []
            }
    
    # Find MVP weights
    mvp_weights, mvp_success, mvp_message = _run_single_optimisation(
        portfolio_volatility, initial_guess, args=(covariance_matrix,),
        bounds=bounds, constraints=constraints
    )

    if mvp_success:
        results['weights'] = mvp_weights
        results['success'] = True
        results['message'] = mvp_message
        
        if verbose:
            print("\nMinimum Variance Portfolio (MVP):")
            print(f"Return: {results['metrics']['Return']:.2%}")
            print(f"Volatility: {results['metrics']['Volatility']:.2%}")
#        print("Weights:")
#            for i, ticker in enumerate(portfolio_tickers):
#                print(f"  {ticker}: {results['weights'][i]:.2%}") 
 
        # Trace the Efficient Frontier
        if num_assets > 20: # Only trace if many assets for performance
            min_return_for_frontier = results['metrics']['Return']
            max_return_for_frontier = max(annual_returns) + 0.001  # Add a small buffer to ensure the highest return point is included. 
            if min_return_for_frontier >= max_return_for_frontier:
                max_return_for_frontier = min_return_for_frontier + 0.001

            #print("min_return_for_frontier_static: ", min_return_for_frontier_static, "max_return_for_frontier_static: ", max_return_for_frontier_static) 
            target_returns = np.linspace(min_return_for_frontier, max_return_for_frontier, num_frontier_points)

            efficient_frontier_std_devs = [results['metrics']['Volatility']]
            efficient_frontier_returns = [results['metrics']['Return']]
            current_initial_guess_frontier = results['weights']

            failures_in_a_row = 0
            last_achieved_target = min_return_for_frontier
            
            for target_ret in target_returns:
                return_constraint = {'type': 'eq', 'fun': lambda x: portfolio_return(x, annual_returns) - target_ret}
                all_constraints = constraints + [return_constraint]

                frontier_weights, frontier_success, frontier_message = _run_single_optimisation(
                    portfolio_volatility, current_initial_guess_frontier, args=(covariance_matrix,),
                    bounds=bounds, constraints=all_constraints
                )

                #print(f"DEBUG: Static Frontier optimisation for target return {frontier_return:.2%}: Success={frontier_success}, Message={frontier_message}") 
                if frontier_success:
                    frontier_volatility = portfolio_volatility(frontier_weights, covariance_matrix)
                    frontier_return = portfolio_return(frontier_weights, annual_returns)
                    efficient_frontier_std_devs.append(frontier_volatility)
                    efficient_frontier_returns.append(frontier_return)
                    current_initial_guess_frontier = frontier_weights
                    failures_in_a_row = 0
                    last_achieved_target = target_ret
                else:
                    #print(f"Optimisation failed at target return {frontier_return:.2%}: {frontier_message}")
                    failures_in_a_row += 1
                    if failures_in_a_row >= 1:
                        print(f"\nMaximum return target: {last_achieved_target:.2%} (Efficient frontier tracing stopped).")
                        break 

            # Ensures the frontier is drawn smoothly and monotonically left-to-right. 
            optimised_points = sorted(list(zip(efficient_frontier_std_devs, efficient_frontier_returns)))
            results['efficient_frontier_std_devs'] = [p[0] for p in optimised_points]
            results['efficient_frontier_returns'] = [p[1] for p in optimised_points]

    else:
        weights_to_use = initial_guess 
        success_status = False
        message_to_use = f"Optimisation failed: {mvp_message}."
        if verbose:
            print(f"Warning: {message_to_use}")

        results = {
            'weights': weights_to_use,
            'metrics': _calculate_portfolio_metrics_full(weights_to_use, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
            'success': success_status,
            'message': message_to_use
        }

    return results

def calculate_sharpe_portfolio(annual_returns: np.ndarray, 
                               covariance_matrix: np.ndarray, 
                               initial_guess: np.ndarray, 
                               bounds: tuple, 
                               constraints: tuple,
                               daily_returns_df_slice: pd.DataFrame,
                               risk_free_rate: float,
                               verbose: bool = True) -> dict:
    """
    Calculates the Tangency Portfolio (Maximum Sharpe Ratio Portfolio).

    Args:
        annual_returns (np.ndarray): Annualised expected returns of assets.
        covariance_matrix (np.ndarray): Annualised covariance matrix.
        initial_guess (np.ndarray): Initial guess for portfolio weights.
        bounds (tuple): Bounds for each weight.
        constraints (tuple): Constraints for the optimisation.
        daily_returns_df_slice (pd.DataFrame): Daily returns for the lookback period (for metrics).
        risk_free_rate (float): Risk-free rate.
        verbose (bool): If True, prints detailed results.

    Returns:
        dict: A dictionary containing Sharpe portfolio weights and metrics.
    """
    results = {
        'weights': initial_guess,
        'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
        'success': False,
        'message': "Optimisation not attempted or failed"
    }

    # Check if covariance matrix is singular
    if np.linalg.matrix_rank(covariance_matrix) < covariance_matrix.shape[0]:
        message = "Optimisation defaulted to initial weights due to singular covariance matrix."
        if verbose:
            print(f"Warning: {message}")
            return {
                'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
                'success': True, # Treat as success for test reporting, as it defaults to valid weights
                'message': message
            }
        
    sharpe_weights, sharpe_success, sharpe_message = _run_single_optimisation(
        negative_sharpe_ratio, initial_guess, 
        args=(annual_returns, covariance_matrix, risk_free_rate),
        bounds=bounds, constraints=constraints
    )

    if sharpe_success:
        results['weights'] = sharpe_weights
        results['metrics'] = _calculate_portfolio_metrics_full(sharpe_weights, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate)
        results['success'] = True
        results['message'] = sharpe_message

        if verbose:
            print(f"\nTangency Portfolio (Max Sharpe Ratio = {results['metrics'].get('Sharpe Ratio', np.nan):.4f}):")
            print(f"Risk-Free Rate: {risk_free_rate:.2%}")
            print(f"Return: {results['metrics']['Return']:.2%}")
            print(f"Volatility: {results['metrics']['Volatility']:.2%}")
#            print("Weights:")
#            for i, ticker in enumerate(portfolio_tickers):
#                print(f"  {ticker}: {results['weights'][i]:.2%}")
    else:
        weights_to_use = initial_guess 
        success_status = False
        message_to_use = f"Optimisation failed: {sharpe_message}."
        if verbose:
            print(f"Warning: {message_to_use}")

        results = {
            'weights': weights_to_use,
            'metrics': _calculate_portfolio_metrics_full(weights_to_use, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
            'success': success_status,
            'message': message_to_use
        }

    return results

def calculate_sortino_portfolio(annual_returns: np.ndarray, 
                                covariance_matrix: np.ndarray, 
                                initial_guess: np.ndarray, 
                                bounds: tuple, 
                                constraints: tuple,
                                daily_returns_df_slice: pd.DataFrame,
                                risk_free_rate: float,
                                verbose: bool = True) -> dict:
    """
    Calculates the Sortino Ratio Optimised Portfolio.

    Args:
        annual_returns (np.ndarray): Annualised expected returns of assets.
        covariance_matrix (np.ndarray): Annualised covariance matrix.
        initial_guess (np.ndarray): Initial guess for portfolio weights.
        bounds (tuple): Bounds for each weight.
        constraints (tuple): Constraints for the optimisation.
        daily_returns_df_slice (pd.DataFrame): Daily returns for the lookback period.
        risk_free_rate (float): Risk-free rate.
        verbose (bool): If True, prints detailed results.

    Returns:
        dict: A dictionary containing Sortino portfolio weights and metrics.
    """
    results = {
        'weights': initial_guess,
        'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
        'success': False,
        'message': "Optimisation not attempted or failed"
    }

    # Check if covariance matrix is singular
    if np.linalg.matrix_rank(covariance_matrix) < covariance_matrix.shape[0]:
        message = "Optimisation defaulted to initial weights due to singular covariance matrix."
        if verbose:
            print(f"Warning: {message}")
            return {
                'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
                'success': True, # Treat as success for test reporting, as it defaults to valid weights
                'message': message
            }
        
    sortino_weights, sortino_success, sortino_message = _run_single_optimisation(
        negative_sortino_ratio, initial_guess, 
        args=(annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
        bounds=bounds, constraints=constraints
    )

    if sortino_success:
        results['weights'] = sortino_weights
        results['metrics'] = _calculate_portfolio_metrics_full(sortino_weights, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate)
        results['success'] = True
        results['message'] = sortino_message

        if verbose:
            print(f"\nSortino Portfolio (Max Sortino Ratio = {results['metrics'].get('Sortino Ratio', np.nan):.4f}):")
            print(f"Risk-Free Rate: {risk_free_rate:.2%}")
            print(f"Return: {results['metrics']['Return']:.2%}")
            print(f"Volatility: {results['metrics']['Volatility']:.2%}")
#            print("Weights:")
#            for i, ticker in enumerate(portfolio_tickers): 
#                print(f"  {ticker}: {results['weights'][i]:.2%}")
    else:
        weights_to_use = initial_guess 
        success_status = False
        message_to_use = f"Optimisation failed: {sortino_message}."
        if verbose:
            print(f"Warning: {message_to_use}")

        results = {
            'weights': weights_to_use,
            'metrics': _calculate_portfolio_metrics_full(weights_to_use, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate),
            'success': success_status,
            'message': message_to_use
        }

    return results

def calculate_mvsk_portfolio(annual_returns: np.ndarray, 
                             covariance_matrix: np.ndarray, 
                             initial_guess: np.ndarray, 
                             bounds: tuple, 
                             constraints: tuple,
                             daily_returns_df_slice: pd.DataFrame,
                             risk_free_rate: float,
                             lambda_s: float,
                             lambda_k: float,
                             verbose: bool = True) -> dict:
    """
    Calculates the Mean-Variance-Skewness-Kurtosis (MVSK) Optimised Portfolio.

    Args:
        annual_returns (np.ndarray): Annualised expected returns of assets.
        covariance_matrix (np.ndarray): Annualised covariance matrix.
        initial_guess (np.ndarray): Initial guess for portfolio weights.
        bounds (tuple): Bounds for each weight.
        constraints (tuple): Constraints for the optimisation.
        daily_returns_df_slice (pd.DataFrame): Daily returns for the lookback period.
        risk_free_rate (float): Risk-free rate.
        lambda_s (float): Coefficient for skewness.
        lambda_k (float): Coefficient for kurtosis.
        verbose (bool): If True, prints detailed results.

    Returns:
        dict: A dictionary containing MVSK portfolio weights and metrics.
    """
    results = {
        'weights': initial_guess,
        'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate, lambda_s, lambda_k),
        'success': False,
        'message': "Optimisation not attempted or failed"
    }

    # Check if covariance matrix is singular
    if np.linalg.matrix_rank(covariance_matrix) < covariance_matrix.shape[0]:
        message = "Optimisation defaulted to initial weights due to singular covariance matrix."
        if verbose:
            print(f"Warning: {message}")
            return {
                'metrics': _calculate_portfolio_metrics_full(initial_guess, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate, lambda_s, lambda_k),
                'success': True, # Treat as success for test reporting, as it defaults to valid weights
                'message': message
            }
        
    mvsk_weights, mvsk_success, mvsk_message = _run_single_optimisation(
        negative_mvsk_utility, initial_guess, 
        args=(annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate, lambda_s, lambda_k),
        bounds=bounds, constraints=constraints
    )

    if mvsk_success:
        results['weights'] = mvsk_weights
        results['metrics'] = _calculate_portfolio_metrics_full(mvsk_weights, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate, lambda_s, lambda_k)
        results['success'] = True
        results['message'] = mvsk_message

        if verbose:
            print(f"\nMean-Variance-Skewness-Kurtosis Portfolio:")
            print(f"Return: {results['metrics']['Return']:.2%}")
            print(f"Volatility: {results['metrics']['Volatility']:.2%}")
            print(f"Skewness: {results['metrics']['Skewness']:.4f}")
            print(f"Kurtosis: {results['metrics']['Kurtosis']:.4f}")
#            print("Weights:")
#            for i, ticker in enumerate(portfolio_tickers):
#                print(f"  {ticker}: {results['weights'][i]:.2%}")
    else:
        weights_to_use = initial_guess 
        success_status = False
        message_to_use = f"Optimisation failed: {mvsk_message}."
        if verbose:
            print(f"Warning: {message_to_use}")

        results = {
            'weights': weights_to_use,
            'metrics': _calculate_portfolio_metrics_full(weights_to_use, annual_returns, daily_returns_df_slice, covariance_matrix, risk_free_rate, lambda_s, lambda_k),
            'success': success_status,
            'message': message_to_use
        }

    return results

# Function to perform static optimisation
def perform_static_optimisation(annual_returns_array, static_annualised_covariance_matrix, initial_guess, bounds, constraints, daily_returns, risk_free_rate, num_assets, num_frontier_points, lambda_s, lambda_k, feature_toggles):
    """
    Performs static portfolio optimisations (MVP, Sharpe, Sortino, MVSK).

    Args:
        annual_returns_array (np.array): Annualised mean returns for all stocks.
        static_annualised_covariance_matrix (np.array): Static annualised covariance matrix.
        initial_guess (np.array): Initial equal-weighted guess for optimisation.
        bounds (tuple): Bounds for each asset's weight.
        constraints (list): List of dictionaries defining the optimisation constraints.
        daily_returns (pd.DataFrame): DataFrame of daily returns for all stocks.
        risk_free_rate (float): Risk-free rate.
        num_assets (int): Number of assets.
        num_frontier_points (int): Number of points for efficient frontier.
        lambda_s (float): Skewness penalty parameter for MVSK.
        lambda_k (float): Kurtosis penalty parameter for MVSK.
        feature_toggles (dict): Dictionary of feature toggles.

    Returns:
        dict: A dictionary containing the results of each enabled static optimisation.
    """
    
    # Variables to store static optimisation results
    static_results = {
        'mvp': None,
        'sharpe': None,
        'sortino': None,
        'mvsk': None,
        'efficient_frontier_std_devs': [],
        'efficient_frontier_returns': []
    }

    if feature_toggles['RUN_STATIC_PORTFOLIO']:
        print("\n--- OPTIMISATION WITH STATIC COVARIANCE MODEL ---")

        # 0. Calculate Equal-Weighted Portfolio - Static
        if feature_toggles['RUN_EQUAL_WEIGHTED_PORTFOLIO']:
            print("Calculating static equal-weighted portfolio...")
            equal_weights = np.array([1./num_assets] * num_assets)
            
            # Calculate portfolio annual return
            portfolio_annual_return = np.sum(annual_returns_array * equal_weights)
            
            # Calculate portfolio annual standard deviation
            portfolio_annual_std_dev = np.sqrt(np.dot(equal_weights.T, np.dot(static_annualised_covariance_matrix, equal_weights)))
            
            # Calculate Sharpe ratio
            if portfolio_annual_std_dev != 0:
                sharpe_ratio = (portfolio_annual_return - risk_free_rate) / portfolio_annual_std_dev
            else:
                sharpe_ratio = 0.0 # Handle case where std dev is zero

            
            static_results['ewp'] = {
                'weights': equal_weights,
                'Return': portfolio_annual_return,
                'Volatility': portfolio_annual_std_dev,
                'Sharpe Ratio': sharpe_ratio,
                'success': True,
                'message': 'Static equal-weighted portfolio calculated successfully.'
            }
            print(f"Static Equal-Weighted Portfolio (EWP) - Annual return: {portfolio_annual_return:.2%}, annual volatility: {portfolio_annual_std_dev:.2%}, Sharpe ratio: {sharpe_ratio:.4f}")
        
        # 1. Find Minimum Variance Portfolio (MVP) - Static
        if feature_toggles['RUN_MVO_OPTIMISATION']:
            static_results['mvp'] = calculate_mvp_portfolio(
                annual_returns=annual_returns_array,
                covariance_matrix=static_annualised_covariance_matrix,
                initial_guess=initial_guess,
                bounds=bounds,
                constraints=constraints,
                daily_returns_df_slice=daily_returns,
                risk_free_rate=risk_free_rate,
                num_assets=num_assets,
                num_frontier_points=num_frontier_points,
                verbose=True
            )
            if static_results['mvp']['success'] and num_assets > 20:
                static_results['efficient_frontier_std_devs'] = static_results['mvp']['efficient_frontier_std_devs']
                static_results['efficient_frontier_returns'] = static_results['mvp']['efficient_frontier_returns']

        # 2. Find Tangency Portfolio (Maximum Sharpe Ratio Portfolio) - Static
        if feature_toggles['RUN_SHARPE_OPTIMISATION']:
            static_results['sharpe'] = calculate_sharpe_portfolio(
                annual_returns=annual_returns_array,
                covariance_matrix=static_annualised_covariance_matrix,
                initial_guess=initial_guess,
                bounds=bounds,
                constraints=constraints,
                daily_returns_df_slice=daily_returns,
                risk_free_rate=risk_free_rate,
                verbose=True
            )

        # 3. Find Sortino Ratio Optimised Portfolio - Static
        if feature_toggles['RUN_SORTINO_OPTIMISATION']:
            static_results['sortino'] = calculate_sortino_portfolio(
                annual_returns=annual_returns_array,
                covariance_matrix=static_annualised_covariance_matrix,
                initial_guess=initial_guess,
                bounds=bounds,
                constraints=constraints,
                daily_returns_df_slice=daily_returns,
                risk_free_rate=risk_free_rate,
                verbose=True
            )
        
        # 4. Find MVSK Optimised Portfolio - Static
        if feature_toggles['RUN_MVSK_OPTIMISATION']:
            static_results['mvsk'] = calculate_mvsk_portfolio(
                annual_returns=annual_returns_array,
                covariance_matrix=static_annualised_covariance_matrix,
                initial_guess=initial_guess,
                bounds=bounds,
                constraints=constraints,
                daily_returns_df_slice=daily_returns,
                risk_free_rate=risk_free_rate,
                lambda_s=lambda_s,
                lambda_k=lambda_k,
                verbose=True
            )
    return static_results

# Function to perform dynamic optimisation
def perform_dynamic_optimisation(annual_returns_array, daily_returns, initial_guess, bounds, constraints, risk_free_rate, num_assets, num_frontier_points, rolling_window_days, lambda_s, lambda_k, feature_toggles):
    """
    Performs dynamic portfolio optimisations (MVP, Sharpe, Sortino, MVSK) using a rolling window.

    Args:
        annual_returns_array (np.array): Annualised mean returns for all stocks.
        daily_returns (pd.DataFrame): DataFrame of daily returns for all stocks.
        initial_guess (np.array): Initial equal-weighted guess for optimisation.
        bounds (tuple): Bounds for each asset's weight.
        constraints (list): List of dictionaries defining the optimisation constraints.
        risk_free_rate (float): Risk-free rate.
        num_assets (int): Number of assets.
        num_frontier_points (int): Number of points for efficient frontier.
        rolling_window_days (int): Number of days for the rolling covariance window.
        lambda_s (float): Skewness penalty parameter for MVSK.
        lambda_k (float): Kurtosis penalty parameter for MVSK.
        feature_toggles (dict): Dictionary of feature toggles.

    Returns:
        dict: A dictionary containing the results of each enabled dynamic optimisation
              and a flag indicating if dynamic covariance was available.
    """
    dynamic_results = {
        'mvp': None,
        'sharpe': None,
        'sortino': None,
        'mvsk': None,
        'efficient_frontier_std_devs': [],
        'efficient_frontier_returns': [],
        'dynamic_covariance_available': False
    }

    if feature_toggles['RUN_DYNAMIC_PORTFOLIO']:
        if len(daily_returns) < rolling_window_days:
            print(f"\nWarning: Not enough historical data ({len(daily_returns)} days) for a rolling window of {rolling_window_days} days.")
            print("Skipping dynamic covariance optimisation. Please ensure sufficient historical data.")
        else:
            # Get the last valid rolling covariance matrix
            temp_rolling_cov = daily_returns.rolling(window=rolling_window_days).cov() * 252
            rolling_cov_matrix_full = temp_rolling_cov.dropna() # Drop any windows that resulted in NaNs
             #print("rolling_cov_matrix_full: ", rolling_cov_matrix_full)

            if not rolling_cov_matrix_full.empty:
                # Get the last timestamp for which a complete matrix was calculated
                last_timestamp = rolling_cov_matrix_full.index.get_level_values(0).unique()[-1]
                # Select the block corresponding to this last timestamp
                dynamic_annualised_covariance_matrix = rolling_cov_matrix_full.loc[last_timestamp].values
            else:
                print("\nError: No complete rolling covariance matrices found after dropping NaNs for dynamic optimisation.")
                exit() # Return early if no valid dynamic cov
                
            #print("rolling_cov_matrix_full: ", rolling_cov_matrix_full)
            #print("last_timestamp: ", last_timestamp)
            #print("dynamic_annualised_covariance_matrix: ", dynamic_annualised_covariance_matrix)

            # Ensure dynamic_annualised_covariance_matrix is not NaN itself
            if np.isnan(dynamic_annualised_covariance_matrix).any():
                print("\nWarning: The selected dynamic covariance matrix still contains NaN values.")
                print("This might happen if individual stock returns within the last window had NaNs before the .dropna() call.")
                exit() # Return early if dynamic cov has NaNs
            
            dynamic_results['dynamic_covariance_available'] = True
            print(f"\n--- OPTIMISATION WITH DYNAMIC COVARIANCE MODEL (Last {rolling_window_days} Days) ---")

            # 0. Calculate Equal-Weighted Portfolio - Dynamic
            if feature_toggles['RUN_EQUAL_WEIGHTED_PORTFOLIO']:
                print("Calculating dynamic equal-weighted portfolio...")
                equal_weights = np.array([1./num_assets] * num_assets)
                
                # Calculate portfolio annual return
                portfolio_annual_return = np.sum(annual_returns_array * equal_weights)
                
                # Calculate portfolio annual standard deviation
                portfolio_annual_std_dev = np.sqrt(np.dot(equal_weights.T, np.dot(dynamic_annualised_covariance_matrix, equal_weights)))
                
                # Calculate Sharpe ratio
                if portfolio_annual_std_dev != 0:
                    sharpe_ratio = (portfolio_annual_return - risk_free_rate) / portfolio_annual_std_dev
                else:
                    sharpe_ratio = 0.0 # Handle case where std dev is zero

                
                dynamic_results['ewp'] = {
                    'weights': equal_weights,
                    'Return': portfolio_annual_return,
                    'Volatility': portfolio_annual_std_dev,
                    'Sharpe Ratio': sharpe_ratio,
                    'success': True,
                    'message': 'Dynamic equal-weighted portfolio calculated successfully.'
                }
                print(f"Dynamic Equal-Weighted Portfolio (EWP) - Annual return: {portfolio_annual_return:.2%}, annual volatility: {portfolio_annual_std_dev:.2%}, Sharpe ratio: {sharpe_ratio:.4f}")
            
            # 1. Find Minimum Variance Portfolio (MVP) - Dynamic
            if feature_toggles['RUN_MVO_OPTIMISATION']:
                dynamic_results['mvp'] = calculate_mvp_portfolio(
                        annual_returns=annual_returns_array,
                        covariance_matrix=dynamic_annualised_covariance_matrix,
                        initial_guess=initial_guess,
                        bounds=bounds,
                        constraints=constraints,
                        daily_returns_df_slice=daily_returns,
                        risk_free_rate=risk_free_rate,
                        num_assets=num_assets,
                        num_frontier_points=num_frontier_points,
                        verbose=True
                    )
                if dynamic_results['mvp']['success'] and num_assets > 20:
                    dynamic_results['efficient_frontier_std_devs'] = dynamic_results['mvp']['efficient_frontier_std_devs']
                    dynamic_results['efficient_frontier_returns'] = dynamic_results['mvp']['efficient_frontier_returns']

            # 2. Find Tangency Portfolio (Maximum Sharpe Ratio Portfolio) - Dynamic
            if feature_toggles['RUN_SHARPE_OPTIMISATION']:
                dynamic_results['sharpe'] = calculate_sharpe_portfolio(
                                        annual_returns=annual_returns_array,
                                        covariance_matrix=dynamic_annualised_covariance_matrix,
                                        initial_guess=initial_guess,
                                        bounds=bounds,
                                        constraints=constraints,
                                        daily_returns_df_slice=daily_returns,
                                        risk_free_rate=risk_free_rate,
                                        verbose=True)
                                        
            # 3. Find Sortino Ratio Optimised Portfolio - Dynamic
            if feature_toggles['RUN_SORTINO_OPTIMISATION']:
                dynamic_results['sortino'] = calculate_sortino_portfolio(
                        annual_returns=annual_returns_array,
                        covariance_matrix=dynamic_annualised_covariance_matrix,
                        initial_guess=initial_guess,
                        bounds=bounds,
                        constraints=constraints,
                        daily_returns_df_slice=daily_returns,
                        risk_free_rate=risk_free_rate,
                        verbose=True
                    )

            # 4. Find MVSK Optimised Portfolio - Dynamic
            if feature_toggles['RUN_MVSK_OPTIMISATION']:
                dynamic_results['mvsk'] = calculate_mvsk_portfolio(
                        annual_returns=annual_returns_array,
                        covariance_matrix=dynamic_annualised_covariance_matrix,
                        initial_guess=initial_guess,
                        bounds=bounds,
                        constraints=constraints,
                        daily_returns_df_slice=daily_returns,
                        risk_free_rate=risk_free_rate,
                        lambda_s=lambda_s,
                        lambda_k=lambda_k,
                        verbose=True
                )
    return dynamic_results

###
### Monte Carlo section
###

# Unique function to toggle Monte Carlo simulations
def run_monte_carlo_simulation(num_portfolio_mc, num_assets, configured_max_stock_weight, configured_max_sector_weight,
                               stock_sectors, portfolio_tickers, annual_returns_array, daily_returns,
                               static_annualised_covariance_matrix, rolling_window_days, risk_free_rate, feature_toggles):
    """
    Runs Monte Carlo simulations to generate random portfolio combinations.

    Args:
        num_portfolio_mc (int): Number of Monte Carlo simulations to run.
        num_assets (int): The total number of assets in the portfolio.
        configured_max_stock_weight (float): Maximum allowed weight for a single stock from config.
        configured_max_sector_weight (float): Maximum allowed weight for a single sector from config.
        stock_sectors (dict): A dictionary mapping stock tickers to their sectors.
        portfolio_tickers (list): A list of ticker symbols for the assets.
        annual_returns_array (np.array): Annualised mean returns for all stocks.
        daily_returns (pd.DataFrame): DataFrame of daily returns for all stocks.
        static_annualised_covariance_matrix (np.array): The static annualised covariance matrix (as numpy array).
        rolling_window_days (int): Number of days for the rolling covariance window.
        risk_free_rate (float): Risk-free rate.
        feature_toggles (dict): Dictionary of feature toggles.

    Returns:
        tuple: A tuple containing:
            - static_portfolio_points_raw_mc (list): List of dictionaries for static Monte Carlo portfolios.
            - dynamic_portfolio_points_raw_mc (list): List of dictionaries for dynamic Monte Carlo portfolios.
    """
    static_portfolio_points_raw_mc = []
    dynamic_portfolio_points_raw_mc = []

    if feature_toggles['RUN_MONTE_CARLO_SIMULATION']:
        # Determine effective_max_stock_weight and effective_max_sector_weight_for_constraint
        # This logic is duplicated from setup_optimisation_constraints to keep MC self-contained
        effective_max_stock_weight = max(configured_max_stock_weight, 1.0 / num_assets)
        effective_max_sector_weight_for_constraint = configured_max_sector_weight
        if num_assets <= 20:
            effective_max_sector_weight_for_constraint = 1.0 # Allow up to 100% for sectors if few assets

        # Re-create sectors dictionary for Monte Carlo constraint checks
        sectors = {}
        for i, ticker in enumerate(portfolio_tickers):
            sector = stock_sectors.get(ticker)
            if sector:
                if sector not in sectors:
                    sectors[sector] = []
                sectors[sector].append(i) # Store index of stock in portfolio_tickers

        # Calculate the last dynamic covariance matrix if dynamic MC is enabled
        dynamic_annualised_covariance_matrix_mc = None
        if feature_toggles['RUN_DYNAMIC_PORTFOLIO']:
            if len(daily_returns) >= rolling_window_days:
                temp_rolling_cov_mc = daily_returns.rolling(window=rolling_window_days).cov() * 252
                rolling_cov_matrix_full_mc = temp_rolling_cov_mc.dropna()
                if not rolling_cov_matrix_full_mc.empty:
                    last_timestamp_mc = rolling_cov_matrix_full_mc.index.get_level_values(0).unique()[-1]
                    dynamic_annualised_covariance_matrix_mc = rolling_cov_matrix_full_mc.loc[last_timestamp_mc].values
                    if np.isnan(dynamic_annualised_covariance_matrix_mc).any():
                        print("Warning: Dynamic covariance matrix for Monte Carlo contains NaNs. Aborting.")
                        exit()
                else:
                    print("Warning: No complete rolling covariance matrices found for Dynamic Monte Carlo. Aborting.")
                    exit()
            else:
                print(f"Warning: Not enough historical data ({len(daily_returns)} days) for a rolling window of {rolling_window_days} days for Dynamic Monte Carlo. Aborting")
                exit()


        for _ in range(num_portfolio_mc):
            weights = np.random.random(num_assets)
            weights /= np.sum(weights) # Normalise weights to sum to 1

            # Check constraints for MC points (optional, but makes MC region more realistic)
            is_valid_mc_point = True
            if np.any(weights > effective_max_stock_weight):
                is_valid_mc_point = False
            
            # Check sector constraints for MC points
            for sector_name, stock_indices in sectors.items():
                sector_weight = np.sum(weights[stock_indices])
                if sector_weight > effective_max_sector_weight_for_constraint:
                    is_valid_mc_point = False
                    break
            
            if is_valid_mc_point:
                if feature_toggles['RUN_STATIC_PORTFOLIO']:
                    port_metrics_static = _calculate_portfolio_metrics_full(weights, annual_returns_array, daily_returns, static_annualised_covariance_matrix, risk_free_rate)
                    static_portfolio_points_raw_mc.append({
                    'std_dev': port_metrics_static['Volatility'],
                    'return': port_metrics_static['Return'],
                    'weights': {ticker: w for ticker, w in zip(portfolio_tickers, weights)}
                })
                
                if feature_toggles['RUN_DYNAMIC_PORTFOLIO'] and dynamic_annualised_covariance_matrix_mc is not None:
                    port_metrics_dynamic = _calculate_portfolio_metrics_full(weights, annual_returns_array, daily_returns, dynamic_annualised_covariance_matrix_mc, risk_free_rate)
                    dynamic_portfolio_points_raw_mc.append({
                    'std_dev': port_metrics_dynamic['Volatility'],
                    'return': port_metrics_dynamic['Return'],
                    'weights': {ticker: w for ticker, w in zip(portfolio_tickers, weights)}
                })
        
        if feature_toggles['RUN_STATIC_PORTFOLIO']:
            print(f"\nGenerated {len(static_portfolio_points_raw_mc)} valid static random portfolios for Monte Carlo visualisation (out of {num_portfolio_mc} attempts).")
        if feature_toggles['RUN_DYNAMIC_PORTFOLIO']:
            print(f"\nGenerated {len(dynamic_portfolio_points_raw_mc)} valid dynamic random portfolios for Monte Carlo visualisation (out of {num_portfolio_mc} attempts).")

    return static_portfolio_points_raw_mc, dynamic_portfolio_points_raw_mc


###
### Plot section
###

# Plot correlation matrix between assets
def plot_correlation_heatmap(correlation_matrix, output_dir):
    """
    Plots and saves a heatmap of the stock correlation matrix.

    Args:
        correlation_matrix (pd.DataFrame): The correlation matrix of stock returns.
        output_dir (str): The directory to save the plot.
    """
    plt.figure(figsize=(18, 10))
    sns.heatmap(correlation_matrix, cmap="Blues", annot=True, fmt='.2f')
    plt.title('Stock correlation heatmap', fontsize=16)
    plot_path = os.path.join(output_dir, "stocks_heatmap.png")
    plt.savefig(plot_path, bbox_inches='tight', dpi=100)
    plt.close()
    print(f"\nHeatmap saved in {plot_path}")

# Single plot for optimised portfolios, Monte Carlo simulation and efficient frontiers
def plot_efficient_frontier_and_portfolios(
    static_results, dynamic_results, individual_stock_metrics, portfolio_tickers,
    static_portfolio_points_raw_mc, dynamic_portfolio_points_raw_mc,
    output_dir, feature_toggles, num_assets
):
    """
    Plots the efficient frontier, Monte Carlo simulations, individual stocks,
    and optimised portfolios (MVP, Sharpe, Sortino, MVSK).

    Args:
        static_results (dict): Results from static optimisation.
        dynamic_results (dict): Results from dynamic optimisation.
        individual_stock_metrics (list): List of dictionaries with individual stock metrics.
        portfolio_tickers (list): List of ticker symbols for the assets.
        static_portfolio_points_raw_mc (list): List of dictionaries for static Monte Carlo portfolios.
        dynamic_portfolio_points_raw_mc (list): List of dictionaries for dynamic Monte Carlo portfolios.
        output_dir (str): The directory to save the plot.
        feature_toggles (dict): Dictionary of feature toggles.
        num_assets (int): Number of assets in the portfolio.
    """
    RUN_STATIC_PORTFOLIO = feature_toggles['RUN_STATIC_PORTFOLIO']
    RUN_DYNAMIC_PORTFOLIO = feature_toggles['RUN_DYNAMIC_PORTFOLIO']
    RUN_EQUAL_WEIGHTED_PORTFOLIO = feature_toggles['RUN_EQUAL_WEIGHTED_PORTFOLIO']
    RUN_MONTE_CARLO_SIMULATION = feature_toggles['RUN_MONTE_CARLO_SIMULATION']
    RUN_MVO_OPTIMISATION = feature_toggles['RUN_MVO_OPTIMISATION']
    RUN_SHARPE_OPTIMISATION = feature_toggles['RUN_SHARPE_OPTIMISATION']
    RUN_SORTINO_OPTIMISATION = feature_toggles['RUN_SORTINO_OPTIMISATION']
    RUN_MVSK_OPTIMISATION = feature_toggles['RUN_MVSK_OPTIMISATION']

    plt.figure(figsize=(14, 8)) # Larger figure for more elements

    # Plot all Monte-Carlo-simulated portfolio combinations (lighter color, background)
    if RUN_MONTE_CARLO_SIMULATION:
        if RUN_STATIC_PORTFOLIO and static_portfolio_points_raw_mc:
            plt.scatter([p['std_dev'] * 100 for p in static_portfolio_points_raw_mc],
                        [p['return'] * 100 for p in static_portfolio_points_raw_mc],
                        color='blue', marker='o', s=10, alpha=0.5, # More transparent
                        label='Monte Carlo portfolio combinations (Static)')
        if RUN_DYNAMIC_PORTFOLIO and dynamic_portfolio_points_raw_mc and dynamic_results['dynamic_covariance_available']:
            plt.scatter([p['std_dev'] * 100 for p in dynamic_portfolio_points_raw_mc],
                        [p['return'] * 100 for p in dynamic_portfolio_points_raw_mc],
                        color='red', marker='o', s=10, alpha=0.5, # More transparent
                        label='Monte Carlo portfolio combinations (Dynamic)')
    
    # Plot the Efficient Frontier line (Static Covariance)
    if RUN_STATIC_PORTFOLIO and RUN_MVO_OPTIMISATION and num_assets > 20 and static_results['mvp'] and static_results['efficient_frontier_std_devs']:
        plt.plot([s * 100 for s in static_results['efficient_frontier_std_devs']],
                 [r * 100 for r in static_results['efficient_frontier_returns']],
                 color='blue', linestyle='-', linewidth=2, label='Efficient frontier (Static)')

    # Plot the Efficient Frontier line (Dynamic Covariance)
    if RUN_DYNAMIC_PORTFOLIO and RUN_MVO_OPTIMISATION and num_assets > 20 and dynamic_results['mvp'] and dynamic_results['efficient_frontier_std_devs'] and dynamic_results['dynamic_covariance_available']:
        plt.plot([s * 100 for s in dynamic_results['efficient_frontier_std_devs']],
                 [r * 100 for r in dynamic_results['efficient_frontier_returns']],
                 color='red', linestyle='-', linewidth=2, label='Efficient frontier (Dynamic)')


    # Plot individual stocks
    individual_stock_colors_palette = sns.color_palette("deep", n_colors=len(portfolio_tickers)).as_hex()

    texts = []
    for i, stock in enumerate(individual_stock_metrics):
        plot_color = individual_stock_colors_palette[i % len(individual_stock_colors_palette)]
        plt.scatter(stock['annualised_std'] * 100, stock['annual_return'] * 100,
        marker='o', s=100, color=plot_color, edgecolor='black', linewidth=1.5,
        label='_nolegend_'
    )
        x = stock['annualised_std'] * 100
        y = stock['annual_return'] * 100
        texts.append(plt.text(x, y, stock['ticker'], color=plot_color, fontsize=11, ha='center', va='bottom'))
    
    plt.scatter([], [], s=100, color='grey', label='Stock\'s annualised performance')  # dummy point
    adjust_text(texts,
            only_move={'points': 'xy', 'text': 'xy'},
            expand_text=(2.0, 2.0),
            expand_points=(2.5, 2.5))

    if RUN_STATIC_PORTFOLIO:
        # Plot the EWP (Static)
        if RUN_EQUAL_WEIGHTED_PORTFOLIO and static_results['ewp'] and static_results['ewp']['success']:
            plt.scatter(static_results['ewp']['Volatility'] * 100, static_results['ewp']['Return'] * 100,
                        marker='p', s=200, color='darkblue', edgecolor='darkblue', alpha=0.3, linewidth=1.5,
                        label=f"EWP (Static), Sharpe ratio={static_results['ewp']['Sharpe Ratio']:.2}")
                        
        # Plot the MVP (Static)
        if RUN_MVO_OPTIMISATION and static_results['mvp'] and static_results['mvp']['success']:
            plt.scatter(static_results['mvp']['metrics']['Volatility'] * 100, static_results['mvp']['metrics']['Return'] * 100,
                        marker='*', s=200, color='darkblue', edgecolor='darkblue', alpha=0.3, linewidth=1.5,
                        label='MV (Static)')

        # Plot the Tangency Portfolio (Static)
        if RUN_SHARPE_OPTIMISATION and static_results['sharpe'] and static_results['sharpe']['success']:
            plt.scatter(static_results['sharpe']['metrics']['Volatility'] * 100, static_results['sharpe']['metrics']['Return'] * 100,
                        marker='P', s=200, color='darkblue', edgecolor='darkblue', alpha=0.3, linewidth=1.5,
                        label=f'Tangency (Static), Sharpe ratio={static_results["sharpe"]["metrics"]["Sharpe Ratio"]:.2}')

        # Plot the Sortino Portfolio (Static)
        if RUN_SORTINO_OPTIMISATION and static_results['sortino'] and static_results['sortino']['success']:
            plt.scatter(static_results['sortino']['metrics']['Volatility'] * 100, static_results['sortino']['metrics']['Return'] * 100,
                        marker='o', s=200, color='darkblue', edgecolor='darkblue', alpha=0.3, linewidth=1.5,
                        label=f'Sortino (Static), Sortino ratio={static_results["sortino"]["metrics"]["Sortino Ratio"]:.2}')
        
        # Plot the MVSK Portfolio (Static)
        if RUN_MVSK_OPTIMISATION and static_results['mvsk'] and static_results['mvsk']['success']:
            plt.scatter(static_results['mvsk']['metrics']['Volatility'] * 100, static_results['mvsk']['metrics']['Return'] * 100,
                        marker='^', s=200, color='darkblue', edgecolor='darkblue', alpha=0.3, linewidth=1.5,
                        label=f'MVSK (Static)')


    # Plot the MVP (Dynamic) if available and enabled
    if RUN_DYNAMIC_PORTFOLIO and dynamic_results['dynamic_covariance_available']:
        # Plot the EWP (Dynamic)
        if RUN_EQUAL_WEIGHTED_PORTFOLIO and dynamic_results['ewp'] and dynamic_results['ewp']['success']:
            plt.scatter(dynamic_results['ewp']['Volatility'] * 100, dynamic_results['ewp']['Return'] * 100,
                        marker='p', s=200, color='red', edgecolor='red', alpha=0.3, linewidth=1.5,
                        label=f"EWP (Dynamic), Sharpe ratio={dynamic_results['ewp']['Sharpe Ratio']:.2}")
                        
        # Plot the MVP (Dynamic)
        if RUN_MVO_OPTIMISATION and dynamic_results['mvp'] and dynamic_results['mvp']['success']:
            plt.scatter(dynamic_results['mvp']['metrics']['Volatility'] * 100, dynamic_results['mvp']['metrics']['Return'] * 100,
                        marker='*', s=200, color='red', edgecolor='red', alpha=0.3, linewidth=1.5,
                        label='MV (Dynamic)')

        # Plot the Tangency Portfolio (Dynamic)
        if RUN_SHARPE_OPTIMISATION and dynamic_results['sharpe'] and dynamic_results['sharpe']['success']:
            plt.scatter(dynamic_results['sharpe']['metrics']['Volatility'] * 100, dynamic_results['sharpe']['metrics']['Return'] * 100,
                        marker='P', s=200, color='red', edgecolor='red', alpha=0.3, linewidth=1.5,
                        label=f'Tangency (Dynamic), Sharpe ratio={dynamic_results["sharpe"]["metrics"]["Sharpe Ratio"]:.2}')

        # Plot the Sortino Portfolio (Dynamic)
        if RUN_SORTINO_OPTIMISATION and dynamic_results['sortino'] and dynamic_results['sortino']['success']:
            plt.scatter(dynamic_results['sortino']['metrics']['Volatility'] * 100, dynamic_results['sortino']['metrics']['Return'] * 100,
                        marker='o', s=200, color='red', edgecolor='red', alpha=0.3, linewidth=1.5,
                        label=f'Sortino (Dynamic), Sortino ratio={dynamic_results["sortino"]["metrics"]["Sortino Ratio"]:.2}')

        # Plot the MVSK Portfolio (Dynamic)
        if RUN_MVSK_OPTIMISATION and dynamic_results['mvsk'] and dynamic_results['mvsk']['success']:
            plt.scatter(dynamic_results['mvsk']['metrics']['Volatility'] * 100, dynamic_results['mvsk']['metrics']['Return'] * 100,
                        marker='^', s=200, color='red', edgecolor='red', alpha=0.3, linewidth=1.5,
                        label=f'MVSK (Dynamic)')


    if (RUN_STATIC_PORTFOLIO or RUN_DYNAMIC_PORTFOLIO) and \
       (RUN_MONTE_CARLO_SIMULATION or RUN_MVO_OPTIMISATION or RUN_SHARPE_OPTIMISATION or RUN_SORTINO_OPTIMISATION or RUN_MVSK_OPTIMISATION):
        plt.title('Optimised portfolios', fontsize=16)
        plt.xlabel('Annualised Standard Deviation (Volatility) (%)', fontsize=12)
        plt.ylabel('Annualised Return (%)', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',labelspacing=2)
        plt.tight_layout(rect=[0, 0, 0.88, 1])
        plot_path = os.path.join(output_dir, "optimised_portfolios.png")
        plt.savefig(plot_path, bbox_inches='tight', dpi=100)
        print(f"\nOptimised portfolios saved in {plot_path}")
        plt.close()
    else:
        print("\nYou must choose the type (static or dynamic) and at least one portfolio (Monte Carlo, Mean-Variance, Tangency, Sortino, MVSK) to display anything!")


# Plot configuration
PLOT_CONFIG = {
    'figure_size': (14, 7),
    'line_alpha': 0.6,
    'line_width': 1,
    'marker_size': 2, 
    #'static_colors': ['#ff0404', '#ffa804', '#04ffd1', '#043dff'],
    'static_colors': ['gold', 'blue', 'red', 'green'],
    'dynamic_colors': ['gold', 'blue', 'red', 'green'],
    'static_linestyles': ['None', 'None', 'None', 'None'], 
    'dynamic_linestyles': ['-', '-', '-', '-','--'],
    'static_markers': ['o', 'o', 'o', 'o'],
    'dynamic_markers': ['None', 'None', 'None', 'None'],
    'legend_loc': 'upper left',
    'legend_bbox_to_anchor': (1, 1),
    'grid_linestyle': '--',
    'grid_alpha': 0.7,
    'title_fontsize': 16,
    'label_fontsize': 12,
    'tight_layout_rect': [0, 0, 0.85, 1],
    'save_dpi': 300
}
            
# Backtest plot: cumulated return over backtest period for both optimised portfolios and equal-weighted portfolio
def plot_cumulative_returns(cumulative_returns_history, output_dir, backtest_end_date, feature_toggles, dynamic_covariance_available):
    """
    Plots and saves the cumulative returns of all strategies over the backtest period.

    Args:
        cumulative_returns_history (pd.DataFrame): DataFrame with cumulative returns for each strategy.
        output_dir (str): The directory to save the plot.
        backtest_end_date (pd.Timestamp): The end date of the backtest period.
        feature_toggles (dict): Dictionary of feature toggles.
        dynamic_covariance_available (bool): Flag indicating if dynamic covariance was available.
    """
    RUN_STATIC_PORTFOLIO = feature_toggles['RUN_STATIC_PORTFOLIO']
    RUN_DYNAMIC_PORTFOLIO = feature_toggles['RUN_DYNAMIC_PORTFOLIO']
    RUN_MVO_OPTIMISATION = feature_toggles['RUN_MVO_OPTIMISATION']
    RUN_SHARPE_OPTIMISATION = feature_toggles['RUN_SHARPE_OPTIMISATION']
    RUN_SORTINO_OPTIMISATION = feature_toggles['RUN_SORTINO_OPTIMISATION']
    RUN_MVSK_OPTIMISATION = feature_toggles['RUN_MVSK_OPTIMISATION']

    plt.figure(figsize=PLOT_CONFIG['figure_size'])
    
    # Define the strategies to potentially plot for cumulative returns
    static_strategies = []
    if RUN_SHARPE_OPTIMISATION: static_strategies.append('Static_Sharpe')
    if RUN_MVO_OPTIMISATION: static_strategies.append('Static_MVP')
    if RUN_SORTINO_OPTIMISATION: static_strategies.append('Static_Sortino')
    if RUN_MVSK_OPTIMISATION: static_strategies.append('Static_MVSK')

    dynamic_strategies = []
    if RUN_SHARPE_OPTIMISATION: dynamic_strategies.append('Dynamic_Sharpe')
    if RUN_MVO_OPTIMISATION: dynamic_strategies.append('Dynamic_MVP')
    if RUN_SORTINO_OPTIMISATION: dynamic_strategies.append('Dynamic_Sortino')
    if RUN_MVSK_OPTIMISATION: dynamic_strategies.append('Dynamic_MVSK')
    
    # Plot the Buy & Hold portfolio
    if 'Buy_and_Hold' in cumulative_returns_history.columns and not cumulative_returns_history['Buy_and_Hold'].isnull().all():
        plt.plot(cumulative_returns_history.index, (cumulative_returns_history['Buy_and_Hold']-1)*100, label='Buy & Hold (EWP)', color='black', linestyle='-')
        
    # Plot the Rebalanced Equally-Weighted Portfolio
    if 'Rebalanced_EWP' in cumulative_returns_history.columns and not cumulative_returns_history['Rebalanced_EWP'].isnull().all():
        plt.plot(cumulative_returns_history.index, (cumulative_returns_history['Rebalanced_EWP']-1)*100, label='Rebalanced_EWP', color='darkorange', linestyle='--')


    # Plot static portfolios
    if RUN_STATIC_PORTFOLIO:
        for i, strategy_key in enumerate(static_strategies):
            if strategy_key in cumulative_returns_history.columns and not cumulative_returns_history[strategy_key].isnull().all():
                plt.plot(cumulative_returns_history.index, (cumulative_returns_history[strategy_key] - 1) * 100,
                         label=strategy_key.replace('_', ' '),
                         color=PLOT_CONFIG['static_colors'][i % len(PLOT_CONFIG['static_colors'])],
                         linewidth=PLOT_CONFIG['line_width'],
                         linestyle=PLOT_CONFIG['static_linestyles'][i % len(PLOT_CONFIG['static_linestyles'])],
                         marker=PLOT_CONFIG['static_markers'][i % len(PLOT_CONFIG['static_markers'])],
                         markersize=PLOT_CONFIG['marker_size'],
                         alpha=PLOT_CONFIG['line_alpha'])

    # Plot dynamic portfolios
    if RUN_DYNAMIC_PORTFOLIO and dynamic_covariance_available:
        for i, strategy_key in enumerate(dynamic_strategies):
            if strategy_key in cumulative_returns_history.columns and not cumulative_returns_history[strategy_key].isnull().all():
                plt.plot(cumulative_returns_history.index, (cumulative_returns_history[strategy_key] - 1) * 100,
                         label=strategy_key.replace('_', ' '),
                         color=PLOT_CONFIG['dynamic_colors'][i % len(PLOT_CONFIG['dynamic_colors'])],
                         linewidth=PLOT_CONFIG['line_width'],
                         linestyle=PLOT_CONFIG['dynamic_linestyles'][i % len(PLOT_CONFIG['dynamic_linestyles'])],
                         marker=PLOT_CONFIG['dynamic_markers'][i % len(PLOT_CONFIG['dynamic_markers'])],
                         markersize=PLOT_CONFIG['marker_size'],
                         alpha=PLOT_CONFIG['line_alpha'])

    plt.title('Cumulative portfolio returns over backtest period', fontsize=PLOT_CONFIG['title_fontsize'])
    plt.xlabel('Date', fontsize=PLOT_CONFIG['label_fontsize'])
    plt.ylabel('Cumulative Returns (%)', fontsize=PLOT_CONFIG['label_fontsize'])
    plt.grid(True, linestyle=PLOT_CONFIG['grid_linestyle'], alpha=PLOT_CONFIG['grid_alpha'])
    plt.legend(loc=PLOT_CONFIG['legend_loc'], bbox_to_anchor=PLOT_CONFIG['legend_bbox_to_anchor'])
    plt.tight_layout(rect=PLOT_CONFIG['tight_layout_rect'])
    
    # Ensure x-axis limits are consistent with backtest period
    if not cumulative_returns_history.empty:
        plt.xlim(cumulative_returns_history.index.min(), backtest_end_date)
        
    plot_path = os.path.join(output_dir, "cumulative_returns.png")
    plt.savefig(plot_path, bbox_inches='tight', dpi=PLOT_CONFIG['save_dpi'])
    plt.close()
    print(f"\nCumulative portfolio returns over backtest period saved in {plot_path}")
            
# Backtest plot: difference between optimised portfolios and equal-weighted portfolio
def plot_benchmark_difference(cumulative_returns_history, output_dir, backtest_end_date, feature_toggles, dynamic_covariance_available):
    """
    Plots and saves the difference of each strategy's cumulative returns relative to the Buy & Hold benchmark.

    Args:
        cumulative_returns_history (pd.DataFrame): DataFrame with cumulative returns for each strategy.
        output_dir (str): The directory to save the plot.
        backtest_end_date (pd.Timestamp): The end date of the backtest period.
        feature_toggles (dict): Dictionary of feature toggles.
        dynamic_covariance_available (bool): Flag indicating if dynamic covariance was available.
    """
    RUN_STATIC_PORTFOLIO = feature_toggles['RUN_STATIC_PORTFOLIO']
    RUN_DYNAMIC_PORTFOLIO = feature_toggles['RUN_DYNAMIC_PORTFOLIO']
    RUN_MVO_OPTIMISATION = feature_toggles['RUN_MVO_OPTIMISATION']
    RUN_SHARPE_OPTIMISATION = feature_toggles['RUN_SHARPE_OPTIMISATION']
    RUN_SORTINO_OPTIMISATION = feature_toggles['RUN_SORTINO_OPTIMISATION']
    RUN_MVSK_OPTIMISATION = feature_toggles['RUN_MVSK_OPTIMISATION']

    if 'Buy_and_Hold' not in cumulative_returns_history.columns or cumulative_returns_history['Buy_and_Hold'].isnull().all():
        print("Cannot plot benchmark difference: 'Buy_and_Hold' data is missing or all NaN.")
        return
        
    if 'Rebalanced_EWP' not in cumulative_returns_history.columns or cumulative_returns_history['Rebalanced_EWP'].isnull().all():
        print("Cannot plot benchmark difference: 'Rebalanced_EWP' data is missing or all NaN.")
        return    

    plt.figure(figsize=PLOT_CONFIG['figure_size'])
    
    # Define the strategies to potentially plot
    static_strategies = []
    if RUN_SHARPE_OPTIMISATION: static_strategies.append('Static_Sharpe')
    if RUN_MVO_OPTIMISATION: static_strategies.append('Static_MVP')
    if RUN_SORTINO_OPTIMISATION: static_strategies.append('Static_Sortino')
    if RUN_MVSK_OPTIMISATION: static_strategies.append('Static_MVSK')

    dynamic_strategies = []
    if RUN_SHARPE_OPTIMISATION: dynamic_strategies.append('Dynamic_Sharpe')
    if RUN_MVO_OPTIMISATION: dynamic_strategies.append('Dynamic_MVP')
    if RUN_SORTINO_OPTIMISATION: dynamic_strategies.append('Dynamic_Sortino')
    if RUN_MVSK_OPTIMISATION: dynamic_strategies.append('Dynamic_MVSK')
    

    # Plot static portfolios
    if RUN_STATIC_PORTFOLIO and static_strategies:
        for i, strategy_key in enumerate(static_strategies):
            if strategy_key in cumulative_returns_history.columns and not cumulative_returns_history[strategy_key].isnull().all():
                ratio = (cumulative_returns_history[strategy_key] / cumulative_returns_history['Buy_and_Hold'] - 1) * 100
                plt.plot(cumulative_returns_history.index, ratio, label=strategy_key.replace('_', ' '),
                         color=PLOT_CONFIG['static_colors'][i % len(PLOT_CONFIG['static_colors'])],
                         linewidth=PLOT_CONFIG['line_width'],
                         linestyle=PLOT_CONFIG['static_linestyles'][i % len(PLOT_CONFIG['static_linestyles'])],
                         marker=PLOT_CONFIG['static_markers'][i % len(PLOT_CONFIG['static_markers'])],
                         markersize=PLOT_CONFIG['marker_size'],
                         alpha=PLOT_CONFIG['line_alpha'])

    # Plot dynamic portfolios
    if RUN_DYNAMIC_PORTFOLIO and dynamic_covariance_available and dynamic_strategies:
        for i, strategy_key in enumerate(dynamic_strategies):
            if strategy_key in cumulative_returns_history.columns and not cumulative_returns_history[strategy_key].isnull().all():
                ratio = (cumulative_returns_history[strategy_key] / cumulative_returns_history['Buy_and_Hold'] - 1) * 100
                plt.plot(cumulative_returns_history.index, ratio,
                         label=strategy_key.replace('_', ' '),
                         color=PLOT_CONFIG['dynamic_colors'][i % len(PLOT_CONFIG['dynamic_colors'])],
                         linewidth=PLOT_CONFIG['line_width'],
                         linestyle=PLOT_CONFIG['dynamic_linestyles'][i % len(PLOT_CONFIG['dynamic_linestyles'])],
                         marker=PLOT_CONFIG['dynamic_markers'][i % len(PLOT_CONFIG['dynamic_markers'])],
                         markersize=PLOT_CONFIG['marker_size'],
                         alpha=PLOT_CONFIG['line_alpha'])

    # Plot the Rebalanced Equally-Weighted Portfolio
    if 'Rebalanced_EWP' in cumulative_returns_history.columns and not cumulative_returns_history['Rebalanced_EWP'].isnull().all():
        ratio = (cumulative_returns_history['Rebalanced_EWP'] / cumulative_returns_history['Buy_and_Hold'] - 1) * 100
        plt.plot(cumulative_returns_history.index, ratio, label='Rebalanced_EWP', color='darkorange', linestyle='-')
        
    plt.title('Benchmark difference', fontsize=PLOT_CONFIG['title_fontsize'])
    plt.xlabel('Date', fontsize=PLOT_CONFIG['label_fontsize'])
    plt.ylabel('Cumulative returns ratio (%)', fontsize=PLOT_CONFIG['label_fontsize'])
    plt.grid(True, linestyle=PLOT_CONFIG['grid_linestyle'], alpha=PLOT_CONFIG['grid_alpha'])
    plt.legend(loc=PLOT_CONFIG['legend_loc'], bbox_to_anchor=PLOT_CONFIG['legend_bbox_to_anchor'])
    plt.tight_layout(rect=PLOT_CONFIG['tight_layout_rect'])
    
    # Ensure x-axis limits are consistent with backtest period
    if not cumulative_returns_history.empty:
        plt.xlim(cumulative_returns_history.index.min(), backtest_end_date)
        
    plot_path = os.path.join(output_dir, "benchmark_difference.png")
    plt.savefig(plot_path, bbox_inches='tight', dpi=300)
    plt.close()
    print(f"\nBenchmark difference saved in {plot_path}")

# Backtest plot: difference between static and dynamic cases
def plot_dynamic_vs_static_difference(cumulative_returns_history, output_dir, backtest_end_date, feature_toggles, dynamic_covariance_available):
    """
    Plots and saves the difference of each static and dynamic strategy's cumulative returns.

    Args:
        cumulative_returns_history (pd.DataFrame): DataFrame with cumulative returns for each strategy.
        output_dir (str): The directory to save the plot.
        backtest_end_date (pd.Timestamp): The end date of the backtest period.
        feature_toggles (dict): Dictionary of feature toggles.
        dynamic_covariance_available (bool): Flag indicating if dynamic covariance was available.
    """
    RUN_STATIC_PORTFOLIO = feature_toggles['RUN_STATIC_PORTFOLIO']
    RUN_DYNAMIC_PORTFOLIO = feature_toggles['RUN_DYNAMIC_PORTFOLIO']
    RUN_MVO_OPTIMISATION = feature_toggles['RUN_MVO_OPTIMISATION']
    RUN_SHARPE_OPTIMISATION = feature_toggles['RUN_SHARPE_OPTIMISATION']
    RUN_SORTINO_OPTIMISATION = feature_toggles['RUN_SORTINO_OPTIMISATION']
    RUN_MVSK_OPTIMISATION = feature_toggles['RUN_MVSK_OPTIMISATION']
    
    plt.figure(figsize=PLOT_CONFIG['figure_size'])
    
    # Define the strategies to potentially plot
    static_strategies = []
    if RUN_SHARPE_OPTIMISATION: static_strategies.append('Static_Sharpe')
    if RUN_MVO_OPTIMISATION: static_strategies.append('Static_MVP')
    if RUN_SORTINO_OPTIMISATION: static_strategies.append('Static_Sortino')
    if RUN_MVSK_OPTIMISATION: static_strategies.append('Static_MVSK')

    dynamic_strategies = []
    if RUN_SHARPE_OPTIMISATION: dynamic_strategies.append('Dynamic_Sharpe')
    if RUN_MVO_OPTIMISATION: dynamic_strategies.append('Dynamic_MVP')
    if RUN_SORTINO_OPTIMISATION: dynamic_strategies.append('Dynamic_Sortino')
    if RUN_MVSK_OPTIMISATION: dynamic_strategies.append('Dynamic_MVSK')
    
    # Plot difference between static and dynamic strategies using the for loop
    for i, static_key in enumerate(static_strategies):
        dynamic_key = static_key.replace("Static", "Dynamic")
        if static_key in cumulative_returns_history.columns and dynamic_key in cumulative_returns_history.columns and \
           not cumulative_returns_history[static_key].isnull().all() and not cumulative_returns_history[dynamic_key].isnull().all():
            ratio = (cumulative_returns_history[dynamic_key] / cumulative_returns_history[static_key] - 1) * 100
            plt.plot(cumulative_returns_history.index, ratio,
                     label=f"{dynamic_key.replace('_', ' ')} vs {static_key.replace('_', ' ')}",
                     color=PLOT_CONFIG['static_colors'][i % len(PLOT_CONFIG['static_colors'])],
                     linewidth=PLOT_CONFIG['line_width'],
                     linestyle=PLOT_CONFIG['dynamic_linestyles'][i % len(PLOT_CONFIG['dynamic_linestyles'])],
                     alpha=PLOT_CONFIG['line_alpha'])


    plt.title('Dynamic vs. Static return difference', fontsize=PLOT_CONFIG['title_fontsize'])
    plt.xlabel('Date', fontsize=PLOT_CONFIG['label_fontsize'])
    plt.ylabel('Return Difference (%)', fontsize=PLOT_CONFIG['label_fontsize'])
    plt.grid(True, linestyle=PLOT_CONFIG['grid_linestyle'], alpha=PLOT_CONFIG['grid_alpha'])
    plt.legend(loc=PLOT_CONFIG['legend_loc'], bbox_to_anchor=PLOT_CONFIG['legend_bbox_to_anchor'])
    plt.tight_layout(rect=PLOT_CONFIG['tight_layout_rect'])
    
    # Ensure x-axis limits are consistent with backtest period
    if not cumulative_returns_history.empty:
        plt.xlim(cumulative_returns_history.index.min(), backtest_end_date)
        
    plot_path = os.path.join(output_dir, "dynamic_vs_static_difference.png")
    plt.savefig(plot_path, bbox_inches='tight', dpi=PLOT_CONFIG['save_dpi'])
    plt.close()
    print(f"\nDynamic vs. Static difference saved in {plot_path}")
    
# Define a master plotting function
def plot_all_results(
    static_annualised_correlation_matrix, output_dir,
    static_results, dynamic_results, individual_stock_metrics, portfolio_tickers,
    static_portfolio_points_raw_mc, dynamic_portfolio_points_raw_mc,
    feature_toggles, num_assets,
    cumulative_returns_history=None, backtest_end_date=None # New parameters for backtest plots
):
    """
    Orchestrates all main plotting functions.

    Args:
        static_annualised_correlation_matrix (pd.DataFrame): Static annualised correlation matrix.
        output_dir (str): The directory to save plots.
        static_results (dict): Results from static optimisation.
        dynamic_results (dict): Results from dynamic optimisation.
        individual_stock_metrics (list): List of dictionaries with individual stock metrics.
        portfolio_tickers (list): List of ticker symbols for the assets.
        static_portfolio_points_raw_mc (list): List of dictionaries for static Monte Carlo portfolios.
        dynamic_portfolio_points_raw_mc (list): List of dictionaries for dynamic Monte Carlo portfolios.
        feature_toggles (dict): Dictionary of feature toggles.
        num_assets (int): Number of assets in the portfolio.
        cumulative_returns_history (pd.DataFrame, optional): DataFrame with cumulative returns history from backtest.
        backtest_end_date (pd.Timestamp, optional): The end date of the backtest period.
    """
    # Plot Correlation Heatmap
    plot_correlation_heatmap(static_annualised_correlation_matrix, output_dir)

    # Plot Efficient Frontier and Portfolios
    plot_efficient_frontier_and_portfolios(
        static_results, dynamic_results, individual_stock_metrics, portfolio_tickers,
        static_portfolio_points_raw_mc, dynamic_portfolio_points_raw_mc,
        output_dir, feature_toggles, num_assets
    )

    # Plot Backtest Results if available and enabled
    if feature_toggles['RUN_BACKTEST'] and cumulative_returns_history is not None and not cumulative_returns_history.empty:
        print("\nPlotting backtest results...")
        plot_cumulative_returns(cumulative_returns_history, output_dir, backtest_end_date, feature_toggles, dynamic_results['dynamic_covariance_available'])
        plot_benchmark_difference(cumulative_returns_history, output_dir, backtest_end_date, feature_toggles, dynamic_results['dynamic_covariance_available'])
        plot_dynamic_vs_static_difference(cumulative_returns_history, output_dir, backtest_end_date, feature_toggles, dynamic_results['dynamic_covariance_available'])
    

###
### Output section
###

# Write all metrics and optimised weights to a csv file
def write_portfolio_weights_to_csv(
    filepath: str,
    portfolio_type: str,
    optimisation_type: str,
    metrics: dict,
    weights: list,
    portfolio_tickers: list
):
    """
    Writes portfolio optimisation results (metrics and weights) to a CSV file.

    Args:
        filename (str): The name of the CSV file to write to.
        portfolio_type (str): Describes the portfolio (e.g., "Static Covariance", "Dynamic Covariance").
        optimisation_type (str): The type of optimisation (e.g., "Minimum Variance", "Tangency", "Sortino", "MVSK").
        metrics (dict): A dictionary of key metrics (Return, Volatility).
        weights (list): A list of numerical weights for each ticker.
        portfolio_tickers (list): A list of ticker symbols corresponding to the weights.
    """
    
    # Define all possible metric headers that might appear across any portfolio type
    all_possible_metric_keys = [
        "Sharpe Ratio",
        "Sortino Ratio",
        "Skewness",
        "Kurtosis"
    ]
    
    # Prepare header for weights (e.g., 'Weight_AAPL', 'Weight_MSFT')
    weight_headers = [f"{ticker}" for ticker in portfolio_tickers]

    # Define the base headers that will always be present
    base_headers = ["Portfolio Type", "Optimisation Type", "Return", "Volatility"]

    # Combine all headers into a single, fixed list
    all_headers = base_headers + all_possible_metric_keys + weight_headers


    # Initialise with all keys from `all_headers` to ensure consistency
    row_data = {header: '' for header in all_headers}
    
    # Prepare the data row
    row_data["Portfolio Type"] = portfolio_type
    row_data["Optimisation Type"] = optimisation_type
    row_data["Return"] = f"{metrics.get('Return', 0.0):.2%}"
    row_data["Volatility"] = f"{metrics.get('Volatility', 0.0):.2%}"

    # Add dynamic metrics to the row data with conditional formatting
    for header_key in all_possible_metric_keys:
        if header_key in metrics:
            value = metrics[header_key]
            row_data[header_key] = f"{value:.4f}"

    # Add weights to the row data
    for i, ticker in enumerate(portfolio_tickers):
        if i < len(weights): # Safety check to prevent index out of bounds
            row_data[f"{ticker}"] = f"{weights[i]:.2%}"
        else:
            # If for some reason weights list is shorter than tickers, fill with empty
            row_data[f"{ticker}"] = ''

    try:
        # Check if file exists to determine if header needs to be written
        file_exists_and_not_empty = os.path.exists(filepath) and os.path.getsize(filepath) > 0

        with open(filepath, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=all_headers)

            if not file_exists_and_not_empty: # Write header only if file is new or being overwritten
                writer.writeheader()
            writer.writerow(row_data)
        print(f"Successfully wrote {optimisation_type} ({portfolio_type}) data to {filepath}")
    except IOError as e:
        print(f"Error writing to CSV file {filepath}: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

# Save everything
def save_optimisation_results(output_filepath, portfolio_tickers, static_results, dynamic_results, feature_toggles):
    """
    Saves the results of static and dynamic portfolio optimisations to a CSV file.

    Args:
        output_filepath (str): The full path to the portfolio results CSV.
        portfolio_tickers (list): List of ticker symbols for the assets.
        static_results (dict): Results from static optimisation.
        dynamic_results (dict): Results from dynamic optimisation.
        feature_toggles (dict): Dictionary of feature toggles.
    """
    RUN_STATIC_PORTFOLIO = feature_toggles['RUN_STATIC_PORTFOLIO']
    RUN_DYNAMIC_PORTFOLIO = feature_toggles['RUN_DYNAMIC_PORTFOLIO']
    RUN_MVO_OPTIMISATION = feature_toggles['RUN_MVO_OPTIMISATION']
    RUN_SHARPE_OPTIMISATION = feature_toggles['RUN_SHARPE_OPTIMISATION']
    RUN_SORTINO_OPTIMISATION = feature_toggles['RUN_SORTINO_OPTIMISATION']
    RUN_MVSK_OPTIMISATION = feature_toggles['RUN_MVSK_OPTIMISATION']

    if RUN_STATIC_PORTFOLIO:
        if RUN_MVO_OPTIMISATION and static_results['mvp'] and static_results['mvp']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Static Covariance",
                optimisation_type="Minimum Variance",
                metrics={
                    "Return": static_results['mvp']['metrics']['Return'],
                    "Volatility": static_results['mvp']['metrics']['Volatility']
                },
                weights=static_results['mvp']['weights'],
                portfolio_tickers=portfolio_tickers
            )

        if RUN_SHARPE_OPTIMISATION and static_results['sharpe'] and static_results['sharpe']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Static Covariance",
                optimisation_type="Tangency (Max Sharpe Ratio)",
                metrics={
                    "Return": static_results['sharpe']['metrics']['Return'],
                    "Volatility": static_results['sharpe']['metrics']['Volatility'],
                    "Sharpe Ratio": static_results['sharpe']['metrics']['Sharpe Ratio']
                },
                weights=static_results['sharpe']['weights'],
                portfolio_tickers=portfolio_tickers
            )

        if RUN_SORTINO_OPTIMISATION and static_results['sortino'] and static_results['sortino']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Static Covariance",
                optimisation_type="Sortino (Max Sortino Ratio)",
                metrics={
                    "Return": static_results['sortino']['metrics']['Return'],
                    "Volatility": static_results['sortino']['metrics']['Volatility'],
                    "Sortino Ratio": static_results['sortino']['metrics']['Sortino Ratio']
                },
                weights=static_results['sortino']['weights'],
                portfolio_tickers=portfolio_tickers
            )

        if RUN_MVSK_OPTIMISATION and static_results['mvsk'] and static_results['mvsk']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Static Covariance",
                optimisation_type="Mean-Variance-Skewness-Kurtosis",
                metrics={
                    "Return": static_results['mvsk']['metrics']['Return'],
                    "Volatility": static_results['mvsk']['metrics']['Volatility'],
                    "Skewness": static_results['mvsk']['metrics']['Skewness'],
                    "Kurtosis": static_results['mvsk']['metrics']['Kurtosis']
                },
                weights=static_results['mvsk']['weights'],
                portfolio_tickers=portfolio_tickers
            )

    if RUN_DYNAMIC_PORTFOLIO and dynamic_results['dynamic_covariance_available']:
        if RUN_MVO_OPTIMISATION and dynamic_results['mvp'] and dynamic_results['mvp']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Dynamic Covariance",
                optimisation_type="Minimum Variance",
                metrics={
                    "Return": dynamic_results['mvp']['metrics']['Return'],
                    "Volatility": dynamic_results['mvp']['metrics']['Volatility']
                },
                weights=dynamic_results['mvp']['weights'],
                portfolio_tickers=portfolio_tickers
            )

        if RUN_SHARPE_OPTIMISATION and dynamic_results['sharpe'] and dynamic_results['sharpe']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Dynamic Covariance",
                optimisation_type="Tangency (Max Sharpe Ratio)",
                metrics={
                    "Return": dynamic_results['sharpe']['metrics']['Return'],
                    "Volatility": dynamic_results['sharpe']['metrics']['Volatility'],
                    "Sharpe Ratio": dynamic_results['sharpe']['metrics']['Sharpe Ratio']
                },
                weights=dynamic_results['sharpe']['weights'],
                portfolio_tickers=portfolio_tickers
            )

        if RUN_SORTINO_OPTIMISATION and dynamic_results['sortino'] and dynamic_results['sortino']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Dynamic Covariance",
                optimisation_type="Sortino (Max Sortino Ratio)",
                metrics={
                    "Return": dynamic_results['sortino']['metrics']['Return'],
                    "Volatility": dynamic_results['sortino']['metrics']['Volatility'],
                    "Sortino Ratio": dynamic_results['sortino']['metrics']['Sortino Ratio']
                },
                weights=dynamic_results['sortino']['weights'],
                portfolio_tickers=portfolio_tickers
            )

        if RUN_MVSK_OPTIMISATION and dynamic_results['mvsk'] and dynamic_results['mvsk']['success']:
            write_portfolio_weights_to_csv(
                filepath=output_filepath,
                portfolio_type="Dynamic Covariance",
                optimisation_type="Mean-Variance-Skewness-Kurtosis",
                metrics={
                    "Return": dynamic_results['mvsk']['metrics']['Return'],
                    "Volatility": dynamic_results['mvsk']['metrics']['Volatility'],
                    "Skewness": dynamic_results['mvsk']['metrics']['Skewness'],
                    "Kurtosis": dynamic_results['mvsk']['metrics']['Kurtosis']
                },
                weights=dynamic_results['mvsk']['weights'],
                portfolio_tickers=portfolio_tickers
            )
    print(f"\nOptimisation data saved to {output_filepath}")

# Write all the backtest results in a csv file
def write_backtest_metrics_to_csv(filepath: str, metrics_summary: dict):
    """
    Writes backtest performance metrics for each strategy to a CSV file.

    Args:
        filepath (str): The full path to the CSV file to write to.
        metrics_summary (dict): A dictionary where keys are strategy names and values are dictionaries
                                containing 'CAGR', 'Annualised Volatility', 'Sharpe ratio', 'Sortino Ratio', and 'Max Drawdown'.
    """
    
    # Define the headers for the backtest metrics CSV
    headers = [
        "Strategy",
        "CAGR",
        "Annualised Volatility",
        "Sharpe Ratio",
        "Sortino Ratio",
        "Max Drawdown"
    ]

    try:
        # Check if file exists to determine if header needs to be written
        file_exists_and_not_empty = os.path.exists(filepath) and os.path.getsize(filepath) > 0

        with open(filepath, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=headers)

            if not file_exists_and_not_empty: # Write header only if file is new or being overwritten
                writer.writeheader()
            
            for strategy_name, metrics in metrics_summary.items():
                row_data = {
                    "Strategy": strategy_name.replace('_', ' '), # Make strategy names readable
                    "CAGR": f"{metrics['CAGR']:.2%}" if pd.notna(metrics['CAGR']) else "", # Store as float for analysis
                    "Annualised Volatility": f"{metrics['Annualised Volatility']:.2%}" if pd.notna(metrics['Annualised Volatility']) else "",
                    "Sharpe Ratio": f"{metrics['Sharpe Ratio']:.4f}" if pd.notna(metrics['Sharpe Ratio']) else "",
                    "Sortino Ratio": f"{metrics['Sortino Ratio']:.4f}" if pd.notna(metrics['Sortino Ratio']) else "",
                    "Max Drawdown": f"{metrics['Max Drawdown']:.2%}" if pd.notna(metrics['Max Drawdown']) else ""
                }
                writer.writerow(row_data)
        print(f"Successfully wrote backtest performance metrics to {filepath}")
    except IOError as e:
        print(f"Error writing backtest metrics to CSV file {filepath}: {e}")
    except Exception as e:
        print(f"An unexpected error occurred while writing backtest metrics: {e}")
        
        
###
### Backtest section
###

# Unique backtest function
def run_backtest(config, all_stock_prices, daily_returns, portfolio_tickers, num_assets,
                 initial_guess, bounds, constraints, risk_free_rate, lambda_s, lambda_k,
                 backtest_start_date, backtest_end_date, rebalancing_frequency,
                 historical_data_window_days):
    """
    Runs the backtest simulation for various portfolio optimisation strategies.

    Args:
        config (dict): The loaded configuration dictionary (for feature toggles).
        all_stock_prices (pd.DataFrame): DataFrame of all stock prices.
        daily_returns (pd.DataFrame): DataFrame of daily returns for all stocks.
        portfolio_tickers (list): List of ticker symbols for the assets.
        num_assets (int): Number of assets.
        initial_guess (np.array): An initial equal-weighted guess for optimisation.
        bounds (tuple): Bounds for each asset's weight.
        constraints (list): List of dictionaries defining the optimisation constraints.
        risk_free_rate (float): Risk-free rate.
        lambda_s (float): Skewness penalty parameter for MVSK.
        lambda_k (float): Kurtosis penalty parameter for MVSK.
        backtest_start_date (pd.Timestamp): The determined backtest start date.
        backtest_end_date (pd.Timestamp): The determined backtest end date.
        rebalancing_frequency (str): Frequency of rebalancing (e.g., 'M' for monthly).
        historical_data_window_days (int): Lookback window for optimisation.

    Returns:
        pd.DataFrame: DataFrame containing cumulative returns history for all strategies.
    """
    RUN_STATIC_PORTFOLIO = config['feature_toggles']['RUN_STATIC_PORTFOLIO']
    RUN_DYNAMIC_PORTFOLIO = config['feature_toggles']['RUN_DYNAMIC_PORTFOLIO']
    RUN_MVO_OPTIMISATION = config['feature_toggles']['RUN_MVO_OPTIMISATION']
    RUN_SHARPE_OPTIMISATION = config['feature_toggles']['RUN_SHARPE_OPTIMISATION']
    RUN_SORTINO_OPTIMISATION = config['feature_toggles']['RUN_SORTINO_OPTIMISATION']
    RUN_MVSK_OPTIMISATION = config['feature_toggles']['RUN_MVSK_OPTIMISATION']
    ROLLING_WINDOW_DAYS = config['portfolio_parameters']['ROLLING_WINDOW_DAYS']

    print(f"\n--- Starting backtest with monthly rebalancing ({backtest_start_date.strftime('%Y-%m-%d')} to {backtest_end_date.strftime('%Y-%m-%d')}) ---")

    if rebalancing_frequency=='ME':
        print(f"\nRebalancing frequency is monthly.")
    else:    
        print(f"\nDEBUG: rebalancing_frequency is unknown or not 'ME'. Current: {rebalancing_frequency}")
    
    # Slice all_stock_prices to the exact backtest period first
    backtest_prices_df = all_stock_prices.loc[backtest_start_date:backtest_end_date]
    if backtest_prices_df.empty:
        print("Error: No stock data available for the specified backtest period. Exiting backtest.")
        exit()

    # Calculate daily returns for this specific backtest period
    backtest_daily_returns_all_stocks = backtest_prices_df.pct_change().dropna()
    if backtest_daily_returns_all_stocks.empty:
        print("Error: Not enough valid daily returns for the backtest period. Exiting backtest.")
        exit()


    # --- Backtest data consistency check ---

    # Generate calendar rebalancing dates based on frequency
    rebalancing_dates_calendar = pd.date_range(start=backtest_start_date, end=backtest_end_date, freq=rebalancing_frequency)

    # Get all unique trading days from your stock prices data
    all_trading_days = all_stock_prices.index.unique()

    # Filter rebalancing_dates_calendar to keep only actual trading days
    rebalancing_dates_on_trading_days = pd.Series(rebalancing_dates_calendar).loc[pd.Series(rebalancing_dates_calendar).isin(all_trading_days)]
    
    if rebalancing_dates_on_trading_days.empty:
        print("Error: No valid trading days found for rebalancing within the specified frequency and date range. Exiting.")
        exit()

    minimum_required_data_end_date = all_stock_prices.index[0] + BDay(historical_data_window_days-1)

    # Further filter rebalancing_dates_on_trading_days based on lookback period requirements
    rebalancing_dates_filtered = [
        d for d in rebalancing_dates_on_trading_days
        if d >= minimum_required_data_end_date # Ensure lookback data starts after initial data point
        and (d - BDay(historical_data_window_days) >= all_stock_prices.index[0])
        and d <= all_stock_prices.index[-1] # Ensure rebalance date is within available data
    ]
    # Convert the filtered list back to a Pandas DatetimeIndex for consistent type and efficient operations
    rebalancing_dates_filtered = pd.DatetimeIndex(rebalancing_dates_filtered)

    #print("\nFirst few rebalancing_dates:", rebalancing_dates_filtered[:5])
    # ---------------------------------------------------------------------------------
    
    if rebalancing_dates_filtered.empty:
        print("Error: No valid rebalancing dates found. Ensure BACKTEST_START_DATE allows for sufficient lookback data.")
        print(f"First available data point: {all_stock_prices.index[0].strftime('%Y-%m-%d')}")
        print(f"Minimum start date for backtest with {historical_data_window_days} days lookback: {minimum_required_data_end_date.strftime('%Y-%m-%d')}")
        exit()

    print(f"Number of rebalancing periods: {len(rebalancing_dates_filtered)}")
    print(f"First rebalancing date: {rebalancing_dates_filtered[0].strftime('%Y-%m-%d')}")
    print(f"Last rebalancing date: {rebalancing_dates_filtered[-1].strftime('%Y-%m-%d')}")
    
    # Ensure the first P&L date is within your backtest_daily_returns_all_stocks index
    pnl_start_date = rebalancing_dates_filtered[0] + BDay(1)
    
    if pnl_start_date not in backtest_daily_returns_all_stocks.index:
        # If it falls on a non-trading day, find the next available trading day
        next_trading_day_loc = backtest_daily_returns_all_stocks.index.searchsorted(pnl_start_date)
        if next_trading_day_loc < len(backtest_daily_returns_all_stocks.index):
            pnl_start_date = backtest_daily_returns_all_stocks.index[next_trading_day_loc]
        else:
            print(f"Error: No trading days available after the first rebalance date {rebalancing_dates_filtered[0]}. Exiting.")
            exit()
    print(f"All portfolios will start tracking from: {pnl_start_date.strftime('%Y-%m-%d')}")

    # Initialise cumulative returns history with the index of the backtest daily returns
    cumulative_returns_history = pd.DataFrame(index=backtest_daily_returns_all_stocks.index[backtest_daily_returns_all_stocks.index >= pnl_start_date])
    initial_portfolio_value = 1.0

#    # Calculate Buy & Hold cumulative returns (buy once and never touch the portfolio again)
#    if not backtest_daily_returns_all_stocks.empty:
#        equal_weights = np.array([1.0 / num_assets] * num_assets)
#        buy_and_hold_portfolio_returns = backtest_daily_returns_all_stocks.dot(equal_weights)
#        cumulative_bh_full = (1 + buy_and_hold_portfolio_returns).cumprod()
#         # Now, slice and rebase Buy & Hold to align with the P&L start date
#        if pnl_start_date in cumulative_bh_full.index:
#            rebase_factor_bh = cumulative_bh_full.loc[pnl_start_date]
#            cumulative_returns_history['Buy_and_Hold'] = cumulative_bh_full.loc[pnl_start_date:] / rebase_factor_bh
#        else:
#            print(f"Warning: EWP data not available for start date {pnl_start_date}. Aborting.")
#            exit()
#    else:
#        print("Warning: Not enough data to calculate Buy & Hold returns. Aborting.")
#        exit()

    initial_prices_bh = backtest_prices_df.loc[pnl_start_date] # Get initial prices at pnl_start_date
    initial_investment_per_stock = (initial_portfolio_value / num_assets) / initial_prices_bh.values  # Calculate initial investment per stock for EWP
    bh_portfolio_value_series = (backtest_prices_df.loc[pnl_start_date:] * initial_investment_per_stock).sum(axis=1) # Calculate the value of each stock over time
    cumulative_returns_history['Buy_and_Hold'] = bh_portfolio_value_series / bh_portfolio_value_series.iloc[0] # Normalise to 1.0 at pnl_start_date
    
    if cumulative_returns_history['Buy_and_Hold'].isnull().all():
        print("Warning: Buy & Hold data is all NaN. Aborting.")
        exit()
    print("Buy & Hold calculated.")
        
    strategy_columns = []
    if RUN_STATIC_PORTFOLIO:
        if RUN_MVO_OPTIMISATION:
            strategy_columns.append('Static_MVP')
        if RUN_SHARPE_OPTIMISATION:    
            strategy_columns.append('Static_Sharpe')
        if RUN_SORTINO_OPTIMISATION:
            strategy_columns.append('Static_Sortino')
        if RUN_MVSK_OPTIMISATION:
            strategy_columns.append('Static_MVSK')

    if RUN_DYNAMIC_PORTFOLIO:
        if RUN_MVO_OPTIMISATION:
            strategy_columns.append('Dynamic_MVP')
        if RUN_SHARPE_OPTIMISATION:
            strategy_columns.append('Dynamic_Sharpe')
        if RUN_SORTINO_OPTIMISATION:
            strategy_columns.append('Dynamic_Sortino')
        if RUN_MVSK_OPTIMISATION:
            strategy_columns.append('Dynamic_MVSK')

    strategy_columns.append('Rebalanced_EWP') # Benchmark
            
    for col in strategy_columns:
        cumulative_returns_history[col] = np.nan
        if not cumulative_returns_history.empty:
            cumulative_returns_history.loc[pnl_start_date, col] = initial_portfolio_value

    last_weights = {name: initial_guess for name in strategy_columns} # Initialise with equal weights or zeros

    for i, rebalance_date in enumerate(rebalancing_dates_filtered):
        # Define lookback period for optimisation (e.g., last 252 trading days)
        lookback_end_date = rebalance_date
        lookback_start_date = lookback_end_date - BDay(historical_data_window_days)

        # Ensure lookback_start_date is not before the first available data point
        lookback_start_date = max(lookback_start_date, all_stock_prices.index[0])
        
        # Get data for the lookback period
        lookback_prices_df = all_stock_prices.loc[lookback_start_date:lookback_end_date]
        
        if lookback_prices_df.empty or len(lookback_prices_df) < 2:
            print(f"Warning: Not enough data for lookback period ending {lookback_end_date.strftime('%Y-%m-%d')}. Aborting.")
            exit()

        lookback_daily_returns_df = lookback_prices_df.pct_change().dropna()

        if lookback_daily_returns_df.empty or len(lookback_daily_returns_df.columns) < num_assets:
            print(f"Warning: Not enough valid daily returns for lookback period ending {lookback_end_date.strftime('%Y-%m-%d')}. Aborting.")
            exit()

        #print(f"\nRolling window: {ROLLING_WINDOW_DAYS}, Lookback period: {len(lookback_daily_returns_df)} days")
        
        # Calculate annualised returns for the lookback period
        lookback_annual_returns_array = lookback_daily_returns_df.mean() * 252
        lookback_annual_returns_array = lookback_annual_returns_array.values

        # --- Perform Optimisations for the current rebalancing period ---
        current_static_cov_matrix = lookback_daily_returns_df.cov() * 252 # Static covariance (for this lookback period)

        #print(f"\n Static cov matrix at {rebalance_date}:\n", current_static_cov_matrix.iloc[:3, :3])
        
        # --- Handle Equally Weighted Rebalanced Portfolio (REWP) ---
        last_weights['Rebalanced_EWP'] = initial_guess

        if RUN_STATIC_PORTFOLIO:                
            if RUN_MVO_OPTIMISATION:
                mvp_results_static_rebalance = calculate_mvp_portfolio(
                    annual_returns=lookback_annual_returns_array,
                    covariance_matrix=current_static_cov_matrix.values,
                    initial_guess=last_weights.get('Static_MVP', initial_guess), # Use last weights as initial guess
                    bounds=bounds,
                    constraints=constraints,
                    daily_returns_df_slice=lookback_daily_returns_df,
                    risk_free_rate=risk_free_rate,
                    num_assets=num_assets,
                    num_frontier_points=config['portfolio_parameters']['NUM_FRONTIER_POINTS'],
                    verbose=False
                )
                if mvp_results_static_rebalance['success']:
                    last_weights['Static_MVP'] = mvp_results_static_rebalance['weights']
                else:
                    print(f"Static MVP optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {mvp_results_static_rebalance['message']}. Using previous weights.")

            if RUN_SHARPE_OPTIMISATION:
                sharpe_results_static_rebalance = calculate_sharpe_portfolio(
                    annual_returns=lookback_annual_returns_array,
                    covariance_matrix=current_static_cov_matrix.values,
                    initial_guess=last_weights.get('Static_Sharpe', initial_guess),
                    bounds=bounds,
                    constraints=constraints,
                    daily_returns_df_slice=lookback_daily_returns_df,
                    risk_free_rate=risk_free_rate,
                    verbose=False
                )
                if sharpe_results_static_rebalance['success']:
                    last_weights['Static_Sharpe'] = sharpe_results_static_rebalance['weights']
                    #print(f"Static weights: {last_weights.get('Static_Sharpe', 'None')}")
                else:
                    print(f"Static Sharpe optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {sharpe_results_static_rebalance['message']}. Using previous weights.")

            if RUN_SORTINO_OPTIMISATION:
                sortino_results_static_rebalance = calculate_sortino_portfolio(
                    annual_returns=lookback_annual_returns_array,
                    covariance_matrix=current_static_cov_matrix.values,
                    initial_guess=last_weights.get('Static_Sortino', initial_guess),
                    bounds=bounds,
                    constraints=constraints,
                    daily_returns_df_slice=lookback_daily_returns_df,
                    risk_free_rate=risk_free_rate,
                    verbose=False
                )
                if sortino_results_static_rebalance['success']:
                   last_weights['Static_Sortino'] = sortino_results_static_rebalance['weights']
                else:
                    print(f"Static Sortino optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {sortino_results_static_rebalance['message']}. Using previous weights.")

            if RUN_MVSK_OPTIMISATION:
                mvsk_results_static_rebalance = calculate_mvsk_portfolio(
                    annual_returns=lookback_annual_returns_array,
                    covariance_matrix=current_static_cov_matrix.values,
                    initial_guess=last_weights.get('Static_MVSK', initial_guess),
                    bounds=bounds,
                    constraints=constraints,
                    daily_returns_df_slice=lookback_daily_returns_df,
                    risk_free_rate=risk_free_rate,
                    lambda_s=lambda_s,
                    lambda_k=lambda_k,
                    verbose=False
                )
                if mvsk_results_static_rebalance['success']:
                    last_weights['Static_MVSK'] = mvsk_results_static_rebalance['weights']
                else:
                    print(f"Static MVSK optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {mvsk_results_static_rebalance['message']}. Using previous weights.")


        if RUN_DYNAMIC_PORTFOLIO:
            # Calculate the effective rolling window size for this specific data slice
            effective_rolling_window = min(ROLLING_WINDOW_DAYS, len(lookback_daily_returns_df))

            if effective_rolling_window >= num_assets: # Ensure enough data points for covariance matrix
                 # Recalculate inputs for the effective rolling window
                dynamic_recent_returns = lookback_daily_returns_df.tail(effective_rolling_window)
                dynamic_cov_matrix = dynamic_recent_returns.cov() * 252
                dynamic_annual_returns_array = dynamic_recent_returns.mean() * 252
                dynamic_annual_returns_array = dynamic_annual_returns_array.values
                
                #print(f"\nDynamic cov matrix at {rebalance_date}:\n", dynamic_cov_matrix.iloc[:3, :3])
                
                # Check if the covariance matrix is valid (not all NaNs)
                if not dynamic_cov_matrix.isnull().values.any():
                    dynamic_cov_matrix_values = dynamic_cov_matrix.values
                    
                    if RUN_MVO_OPTIMISATION:
                        mvp_results_dynamic_rebalance = calculate_mvp_portfolio(
                            annual_returns=dynamic_annual_returns_array,
                            covariance_matrix=dynamic_cov_matrix_values,
                            initial_guess=last_weights.get('Dynamic_MVP', initial_guess),
                            bounds=bounds,
                            constraints=constraints,
                            daily_returns_df_slice=dynamic_recent_returns,
                            risk_free_rate=risk_free_rate,
                            num_assets=num_assets,
                            num_frontier_points=config['portfolio_parameters']['NUM_FRONTIER_POINTS'],
                            verbose=False
                        )
                        if mvp_results_dynamic_rebalance['success']:
                            last_weights['Dynamic_MVP'] = mvp_results_dynamic_rebalance['weights']
                            
                        else:
                            print(f"Dynamic MVP optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {mvp_results_dynamic_rebalance['message']}. Using previous weights.")

                    if RUN_SHARPE_OPTIMISATION:
                        sharpe_results_dynamic_rebalance = calculate_sharpe_portfolio(
                            annual_returns=dynamic_annual_returns_array,
                            covariance_matrix=dynamic_cov_matrix_values,
                            initial_guess=last_weights.get('Dynamic_Sharpe', initial_guess),
                            bounds=bounds,
                            constraints=constraints,
                            daily_returns_df_slice=dynamic_recent_returns,
                            risk_free_rate=risk_free_rate,
                            verbose=False
                        )
                        if sharpe_results_dynamic_rebalance['success']:
                            last_weights['Dynamic_Sharpe'] = sharpe_results_dynamic_rebalance['weights']
                            #print(f"Dynamic weights: {last_weights.get('Dynamic_Sharpe', 'None')}")
                        else:
                            print(f"Dynamic Sharpe optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {sharpe_results_dynamic_rebalance['message']}. Using previous weights.")

                    if RUN_SORTINO_OPTIMISATION:
                        sortino_results_dynamic_rebalance = calculate_sortino_portfolio(
                            annual_returns=dynamic_annual_returns_array,
                            covariance_matrix=dynamic_cov_matrix_values,
                            initial_guess=last_weights.get('Dynamic_Sortino', initial_guess),
                            bounds=bounds,
                            constraints=constraints,
                            daily_returns_df_slice=dynamic_recent_returns,
                            risk_free_rate=risk_free_rate,
                            verbose=False
                        )
                        if sortino_results_dynamic_rebalance['success']:
                            last_weights['Dynamic_Sortino'] = sortino_results_dynamic_rebalance['weights']
                        else:
                            print(f"Dynamic Sortino optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {sortino_results_dynamic_rebalance['message']}. Using previous weights.")

                    if RUN_MVSK_OPTIMISATION:
                        mvsk_results_dynamic_rebalance = calculate_mvsk_portfolio(
                            annual_returns=dynamic_annual_returns_array,
                            covariance_matrix=dynamic_cov_matrix_values,
                            initial_guess=last_weights.get('Dynamic_MVSK', initial_guess),
                            bounds=bounds,
                            constraints=constraints,
                            daily_returns_df_slice=dynamic_recent_returns,
                            risk_free_rate=risk_free_rate,
                            lambda_s=lambda_s,
                            lambda_k=lambda_k,
                            verbose=False
                        )
                        if mvsk_results_dynamic_rebalance['success']:
                            last_weights['Dynamic_MVSK'] = mvsk_results_dynamic_rebalance['weights']
                        else:
                            print(f"Dynamic MVSK optimisation failed at {rebalance_date.strftime('%Y-%m-%d')}: {mvsk_results_dynamic_rebalance['message']}. Using previous weights.")
                            
                else:
                    print(f"Warning: Dynamic covariance matrix contains NaNs for period ending {rebalance_date.strftime('%Y-%m-%d')}. Using previous dynamic weights.")
            else:
                print(f"Warning: Not enough data ({len(lookback_daily_returns_df)} returns) for a dynamic window of {historical_data_window_days} or for covariance matrix calculation ({num_assets} assets) at {rebalance_date.strftime('%Y-%m-%d')}. Using previous dynamic weights.")
        
        # Actual period for which returns will be tracked
        tracking_period_start = rebalance_date + BDay(1)

        # Break the loop as there are no more periods to track.
        if tracking_period_start > backtest_end_date:
            break
            
        # Ends at the next rebalance date or backtest_end_date
        if i < len(rebalancing_dates_filtered) - 1:
            tracking_period_end = rebalancing_dates_filtered[i+1]
        else:
            tracking_period_end = backtest_end_date

        # Ensure tracking_period_start is within the overall P&L start date
        if tracking_period_start < pnl_start_date:
            tracking_period_start = pnl_start_date

        # Get the prices needed to calculate returns for the tracking period.
        forward_prices_slice_start = rebalance_date
        forward_prices_slice_end = tracking_period_end

        forward_prices_df = all_stock_prices.loc[forward_prices_slice_start : forward_prices_slice_end]

        
        if forward_prices_df.empty:
            print(f"Warning: No forward prices for period starting {period_start_date.strftime('%Y-%m-%d')}. Aborting.")
            exit()

        # Calculate returns for the forward period
        forward_returns_df = forward_prices_df.pct_change().dropna()

        # Filter forward_returns_df to only include returns from the actual tracking start date
        forward_returns_df = forward_returns_df.loc[forward_returns_df.index >= tracking_period_start]

        if forward_returns_df.empty:
            error_msg = f"Warning: No valid forward returns for period starting {tracking_period_start.strftime('%Y-%m-%d')} for rebalance date {rebalance_date.strftime('%Y-%m-%d')}. Aborting."
            print(error_msg)
            raise ValueError(error_msg) 

        for strategy_name in strategy_columns:
            weights = last_weights[strategy_name]
            #print(f"\nCalculating returns for {strategy_name} with weights: {weights[:5]}...")  # First 5 weights
            
            # Get the correct starting portfolio value
            if tracking_period_start == pnl_start_date:
                current_portfolio_value = initial_portfolio_value
                #print(f"\n First Starting portfolio value for {strategy_name}: {current_portfolio_value}")
            else:
                # Find the last valid value before the current period start date
                prev_dates = cumulative_returns_history.index[cumulative_returns_history.index < tracking_period_start]
                if not prev_dates.empty:
                    last_valid_value_idx = prev_dates.max() # Get the latest date before period_start_date
                    current_portfolio_value = cumulative_returns_history.loc[last_valid_value_idx, strategy_name]
                    #print(f"\n Second Starting portfolio value for {strategy_name}: {current_portfolio_value}")
                else:
                    print("Warning: no valid period start date")
                    exit()

            # Calculate portfolio daily returns for the forward period with the chosen weights
            portfolio_daily_returns_forward = forward_returns_df.dot(weights)
            #print(f"{strategy_name} portfolio returns for period: {portfolio_daily_returns_forward.head(3).values}")
            # Calculate cumulative returns for the forward period based on the current value
            cumulative_forward_series = (1 + portfolio_daily_returns_forward).cumprod() * current_portfolio_value
            #print("\ncumulative_forward_series: ", cumulative_forward_series)

            # Ensure we only update the dates that are actually in cumulative_returns_history's index
            dates_to_update = cumulative_forward_series.index.intersection(cumulative_returns_history.index)
            #print("\ndates_to_update: ", dates_to_update)
            if not dates_to_update.empty:
                cumulative_returns_history.loc[dates_to_update, strategy_name] = cumulative_forward_series.loc[dates_to_update]
                #print("\ncumulative_returns_history.loc[dates_to_update, strategy_name]: ", cumulative_returns_history.loc[dates_to_update, strategy_name])

    # Forward fill any gaps (e.g., non-trading days)
    cumulative_returns_history = cumulative_returns_history.ffill()

    # Safeguard if initial_portfolio_value is not 1.0.
    for col in cumulative_returns_history.columns: # Loop through all columns including Buy_and_Hold
        if not cumulative_returns_history[col].empty:
            first_val = cumulative_returns_history[col].iloc[0]
            # Only normalise if the first value isn't already the initial_portfolio_value and it's not NaN
            if first_val != initial_portfolio_value and pd.notna(first_val): 
                cumulative_returns_history[col] = cumulative_returns_history[col] / first_val
            elif pd.isna(first_val):
                print("Warning: unlawful normalisation! Aborting.")
                exit()

    print("\nBacktesting complete. Returning cumulative returns for plotting.")
    return cumulative_returns_history

# Calculate all the backtest metrics
def calculate_backtest_metrics(cumulative_returns_history: pd.DataFrame, risk_free_rate: float,
                               backtest_start_date: pd.Timestamp, backtest_end_date: pd.Timestamp) -> dict:
    """
    Calculates key performance metrics for each portfolio strategy over the backtest period.

    Args:
        cumulative_returns_history (pd.DataFrame): DataFrame with cumulative returns for each strategy.
        risk_free_rate (float): The annual risk-free rate.
        backtest_start_date (pd.Timestamp): The actual start date of the backtest.
        backtest_end_date (pd.Timestamp): The actual end date of the backtest.

    Returns:
        dict: A dictionary where keys are strategy names and values are dictionaries
              containing 'CAGR', 'Annualised Volatility', 'Sharpe Ratio', 'Sortino Ratio', and 'Max Drawdown'.
    """
    metrics_summary = {}
    
    # Ensure the index is sorted for accurate calculations
    cumulative_returns_history = cumulative_returns_history.sort_index()

    # Calculate the time span in the backtest period
    actual_start_date = cumulative_returns_history.index.min()
    actual_end_date = cumulative_returns_history.index.max()
    
    # Calculate total trading days in the backtest period
    total_trading_days = len(cumulative_returns_history)
    
    # If total_trading_days is less than 252 (approx 1 year), annualisation might be misleading
    if total_trading_days < 252:
        print(f"Total trading days is {total_trading_days}: less than a year. Annualisation may not be accurate!")
    
    for strategy_name, cumulative_series in cumulative_returns_history.items():
        if cumulative_series.isnull().all():
            metrics_summary[strategy_name] = {
                'CAGR': np.nan,
                'Annualised Volatility': np.nan,
                'Sharpe Ratio': np.nan,
                'Sortino Ratio': np.nan,
                'Max Drawdown': np.nan
            }
            continue

        # Ensure the series starts from 1.0 at its first valid point for correct calculations
        first_valid_idx = cumulative_series.first_valid_index()
        if first_valid_idx is None:
            # Safeguard
            metrics_summary[strategy_name] = {
                'CAGR': np.nan,
                'Annualised Volatility': np.nan,
                'Sharpe Ratio': np.nan,
                'Sortino Ratio': np.nan,
                'Max Drawdown': np.nan
            }
            continue
        
        # Rebase to 1.0 at the first valid index if it's not already
        rebased_series = cumulative_series / cumulative_series.loc[first_valid_idx]
        
        # Calculate daily returns for the strategy
        daily_returns = rebased_series.pct_change().dropna()

        # Skip if not enough data points for meaningful calculations
        if len(daily_returns) < 2:
            metrics_summary[strategy_name] = {
                'CAGR': np.nan,
                'Annualised Volatility': np.nan,
                'Sharpe Ratio': np.nan,
                'Sortino Ratio': np.nan,
                'Max Drawdown': np.nan
            }
            continue

        # CAGR (Compound Annual Growth Rate)
        duration_in_years = (actual_end_date - actual_start_date).days / 365.25
        if duration_in_years > 0:
            cagr = (rebased_series.iloc[-1] / rebased_series.iloc[0])**(1 / duration_in_years) - 1
        else:
            cagr = np.nan # Cannot calculate CAGR for less than a year

        # Annualised volatility
        annualised_volatility = daily_returns.std() * np.sqrt(252)

        # Sharpe Ratio
        avg_daily_return = daily_returns.mean() # Average daily return for the strategy
        annualised_avg_return = avg_daily_return * 252 # Annualised average return for the strategy
        if annualised_volatility > 0:
            sharpe_ratio = (annualised_avg_return - risk_free_rate) / annualised_volatility
        else:
            sharpe_ratio = np.inf if annualised_avg_return > risk_free_rate else np.nan

        # Sortino Ratio
        portfolio_daily_returns_df = pd.DataFrame(daily_returns)
        downside_dev = downside_deviation(np.array([1.0]), portfolio_daily_returns_df, risk_free_rate) # Pass 1.0 as dummy weight for single series
        
        if downside_dev > 0:
            sortino_ratio = (annualised_avg_return - risk_free_rate) / downside_dev
        else:
            sortino_ratio = np.inf if annualised_avg_return > risk_free_rate else np.nan # Handle zero downside deviation

        # Max Drawdown
        max_drawdown = calculate_max_drawdown(rebased_series)

        metrics_summary[strategy_name] = {
            'CAGR': cagr,
            'Annualised Volatility': annualised_volatility,
            'Sharpe Ratio': sharpe_ratio,
            'Sortino Ratio': sortino_ratio,
            'Max Drawdown': max_drawdown
        }
    return metrics_summary    
